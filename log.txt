
==> Audit <==
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  COMMAND   ‚îÇ                     ARGS                     ‚îÇ PROFILE  ‚îÇ         USER         ‚îÇ VERSION ‚îÇ     START TIME      ‚îÇ      END TIME       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ start      ‚îÇ --driver=docker                              ‚îÇ minikube ‚îÇ DESKTOP-T92KEIS\user ‚îÇ v1.37.0 ‚îÇ 24 Nov 25 20:46 EET ‚îÇ                     ‚îÇ
‚îÇ start      ‚îÇ --driver=docker                              ‚îÇ minikube ‚îÇ DESKTOP-T92KEIS\user ‚îÇ v1.37.0 ‚îÇ 24 Nov 25 22:54 EET ‚îÇ 24 Nov 25 23:48 EET ‚îÇ
‚îÇ start      ‚îÇ --driver=docker --kubernetes-version=v1.29.0 ‚îÇ minikube ‚îÇ DESKTOP-T92KEIS\user ‚îÇ v1.37.0 ‚îÇ 24 Nov 25 23:50 EET ‚îÇ                     ‚îÇ
‚îÇ docker-env ‚îÇ --shell powershell                           ‚îÇ minikube ‚îÇ DESKTOP-T92KEIS\user ‚îÇ v1.37.0 ‚îÇ 24 Nov 25 23:50 EET ‚îÇ 24 Nov 25 23:50 EET ‚îÇ
‚îÇ start      ‚îÇ                                              ‚îÇ minikube ‚îÇ DESKTOP-T92KEIS\user ‚îÇ v1.37.0 ‚îÇ 28 Nov 25 00:53 EET ‚îÇ 28 Nov 25 00:54 EET ‚îÇ
‚îÇ start      ‚îÇ                                              ‚îÇ minikube ‚îÇ DESKTOP-T92KEIS\user ‚îÇ v1.37.0 ‚îÇ 04 Dec 25 16:27 EET ‚îÇ 04 Dec 25 16:27 EET ‚îÇ
‚îÇ service    ‚îÇ recipes-frontend -n recipes-app              ‚îÇ minikube ‚îÇ DESKTOP-T92KEIS\user ‚îÇ v1.37.0 ‚îÇ 04 Dec 25 16:28 EET ‚îÇ                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


==> Last Start <==
Log file created at: 2025/12/04 16:27:13
Running on machine: DESKTOP-T92KEIS
Binary: Built with gc go1.24.6 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1204 16:27:13.933131    4912 out.go:360] Setting OutFile to fd 80 ...
I1204 16:27:13.937171    4912 out.go:413] isatty.IsTerminal(80) = true
I1204 16:27:13.937171    4912 out.go:374] Setting ErrFile to fd 84...
I1204 16:27:13.937828    4912 out.go:413] isatty.IsTerminal(84) = true
W1204 16:27:13.974130    4912 root.go:314] Error reading config file at C:\Users\user\.minikube\config\config.json: open C:\Users\user\.minikube\config\config.json: The system cannot find the file specified.
I1204 16:27:14.070829    4912 out.go:368] Setting JSON to false
I1204 16:27:14.079181    4912 start.go:130] hostinfo: {"hostname":"DESKTOP-T92KEIS","uptime":531419,"bootTime":1764327014,"procs":320,"os":"windows","platform":"Microsoft Windows 11 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.22000.2538 Build 22000.2538","kernelVersion":"10.0.22000.2538 Build 22000.2538","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"08df1e27-dac0-416b-b27d-10fb62fa4ac9"}
W1204 16:27:14.079821    4912 start.go:138] gopshost.Virtualization returned error: not implemented yet
I1204 16:27:14.082952    4912 out.go:179] üòÑ  minikube v1.37.0 on Microsoft Windows 11 Pro 10.0.22000.2538 Build 22000.2538
I1204 16:27:14.169605    4912 notify.go:220] Checking for updates...
I1204 16:27:14.247669    4912 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1204 16:27:14.251681    4912 driver.go:421] Setting default libvirt URI to qemu:///system
I1204 16:27:14.367529    4912 docker.go:123] docker version: linux-29.0.1:Docker Desktop 4.52.0 (210994)
I1204 16:27:14.374491    4912 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1204 16:27:19.875250    4912 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (5.5007592s)
I1204 16:27:19.881414    4912 info.go:266] docker info: {ID:6b715b73-8c7d-4995-badd-63c3d53ffa27 Containers:43 ContainersRunning:37 ContainersPaused:0 ContainersStopped:6 Images:17 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:174 OomKillDisable:false NGoroutines:204 SystemTime:2025-12-04 14:27:19.847926871 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:6138593280 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:29.0.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:fcd43222d6b07379a4be9786bda52438f0dd16a1 Expected:} RuncCommit:{ID:v1.3.3-0-gd842d771 Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.29.1-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.3-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.45] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.31] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.28.0] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v1.0.0] map[Name:offload Path:C:\Program Files\Docker\cli-plugins\docker-offload.exe SchemaVersion:0.1.0 ShortDescription:Docker Offload Vendor:Docker Inc. Version:v0.5.20] map[Hidden:true Name:pass Path:C:\Program Files\Docker\cli-plugins\docker-pass.exe SchemaVersion:0.1.0 ShortDescription:Docker Pass Secrets Manager Plugin (beta) Vendor:Docker Inc. Version:v0.0.11] map[Name:sandbox Path:C:\Program Files\Docker\cli-plugins\docker-sandbox.exe SchemaVersion:0.1.0 ShortDescription:Docker Sandbox Vendor:Docker Inc. Version:v0.6.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.3]] Warnings:<nil>}}
I1204 16:27:19.884108    4912 out.go:179] ‚ú®  Using the docker driver based on existing profile
I1204 16:27:19.887427    4912 start.go:304] selected driver: docker
I1204 16:27:19.887515    4912 start.go:918] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1204 16:27:19.887515    4912 start.go:929] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1204 16:27:19.909169    4912 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1204 16:27:20.265471    4912 info.go:266] docker info: {ID:6b715b73-8c7d-4995-badd-63c3d53ffa27 Containers:43 ContainersRunning:37 ContainersPaused:0 ContainersStopped:6 Images:17 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:174 OomKillDisable:false NGoroutines:204 SystemTime:2025-12-04 14:27:20.248102194 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:6138593280 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:29.0.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:fcd43222d6b07379a4be9786bda52438f0dd16a1 Expected:} RuncCommit:{ID:v1.3.3-0-gd842d771 Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.29.1-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.3-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.45] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.31] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.28.0] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v1.0.0] map[Name:offload Path:C:\Program Files\Docker\cli-plugins\docker-offload.exe SchemaVersion:0.1.0 ShortDescription:Docker Offload Vendor:Docker Inc. Version:v0.5.20] map[Hidden:true Name:pass Path:C:\Program Files\Docker\cli-plugins\docker-pass.exe SchemaVersion:0.1.0 ShortDescription:Docker Pass Secrets Manager Plugin (beta) Vendor:Docker Inc. Version:v0.0.11] map[Name:sandbox Path:C:\Program Files\Docker\cli-plugins\docker-sandbox.exe SchemaVersion:0.1.0 ShortDescription:Docker Sandbox Vendor:Docker Inc. Version:v0.6.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.3]] Warnings:<nil>}}
I1204 16:27:20.320502    4912 cni.go:84] Creating CNI manager for ""
I1204 16:27:20.321783    4912 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1204 16:27:20.322332    4912 start.go:348] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1204 16:27:20.323887    4912 out.go:179] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I1204 16:27:20.327623    4912 cache.go:123] Beginning downloading kic base image for docker with docker
I1204 16:27:20.331481    4912 out.go:179] üöú  Pulling base image v0.0.48 ...
I1204 16:27:20.335494    4912 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1204 16:27:20.334498    4912 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48 in local docker daemon
I1204 16:27:20.336073    4912 preload.go:146] Found local preload: C:\Users\user\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I1204 16:27:20.336073    4912 cache.go:58] Caching tarball of preloaded images
I1204 16:27:20.337086    4912 preload.go:172] Found C:\Users\user\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1204 16:27:20.337086    4912 cache.go:61] Finished verifying existence of preloaded tar for v1.34.0 on docker
I1204 16:27:20.338085    4912 profile.go:143] Saving config to C:\Users\user\.minikube\profiles\minikube\config.json ...
I1204 16:27:20.732568    4912 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.48 in local docker daemon, skipping pull
I1204 16:27:20.732568    4912 cache.go:147] gcr.io/k8s-minikube/kicbase:v0.0.48 exists in daemon, skipping load
I1204 16:27:20.733859    4912 cache.go:232] Successfully downloaded all kic artifacts
I1204 16:27:20.735033    4912 start.go:360] acquireMachinesLock for minikube: {Name:mkb3e405ffb35ff57f19f1090207beea089db946 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1204 16:27:20.735033    4912 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I1204 16:27:20.735538    4912 start.go:96] Skipping create...Using existing machine configuration
I1204 16:27:20.736096    4912 fix.go:54] fixHost starting: 
I1204 16:27:20.750710    4912 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1204 16:27:20.809931    4912 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1204 16:27:20.809931    4912 fix.go:138] unexpected machine state, will restart: <nil>
I1204 16:27:20.812229    4912 out.go:252] üîÑ  Restarting existing docker container for "minikube" ...
I1204 16:27:20.829831    4912 cli_runner.go:164] Run: docker start minikube
I1204 16:27:21.496752    4912 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1204 16:27:21.659239    4912 kic.go:430] container "minikube" state is running.
I1204 16:27:21.677153    4912 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1204 16:27:21.738169    4912 profile.go:143] Saving config to C:\Users\user\.minikube\profiles\minikube\config.json ...
I1204 16:27:21.739266    4912 machine.go:93] provisionDockerMachine start ...
I1204 16:27:21.747661    4912 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1204 16:27:21.808393    4912 main.go:141] libmachine: Using SSH client type: native
I1204 16:27:21.823956    4912 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8617c0] 0x864300 <nil>  [] 0s} 127.0.0.1 8526 <nil> <nil>}
I1204 16:27:21.823956    4912 main.go:141] libmachine: About to run SSH command:
hostname
I1204 16:27:21.826675    4912 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I1204 16:27:25.071267    4912 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1204 16:27:25.072354    4912 ubuntu.go:182] provisioning hostname "minikube"
I1204 16:27:25.081260    4912 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1204 16:27:25.148958    4912 main.go:141] libmachine: Using SSH client type: native
I1204 16:27:25.149486    4912 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8617c0] 0x864300 <nil>  [] 0s} 127.0.0.1 8526 <nil> <nil>}
I1204 16:27:25.149486    4912 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1204 16:27:25.388072    4912 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1204 16:27:25.395223    4912 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1204 16:27:25.453568    4912 main.go:141] libmachine: Using SSH client type: native
I1204 16:27:25.454079    4912 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8617c0] 0x864300 <nil>  [] 0s} 127.0.0.1 8526 <nil> <nil>}
I1204 16:27:25.454079    4912 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1204 16:27:25.639266    4912 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1204 16:27:25.641281    4912 ubuntu.go:188] set auth options {CertDir:C:\Users\user\.minikube CaCertPath:C:\Users\user\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\user\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\user\.minikube\machines\server.pem ServerKeyPath:C:\Users\user\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\user\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\user\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\user\.minikube}
I1204 16:27:25.641871    4912 ubuntu.go:190] setting up certificates
I1204 16:27:25.642121    4912 provision.go:84] configureAuth start
I1204 16:27:25.662707    4912 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1204 16:27:25.791883    4912 provision.go:143] copyHostCerts
I1204 16:27:25.835247    4912 exec_runner.go:144] found C:\Users\user\.minikube/ca.pem, removing ...
I1204 16:27:25.835247    4912 exec_runner.go:203] rm: C:\Users\user\.minikube\ca.pem
I1204 16:27:25.836345    4912 exec_runner.go:151] cp: C:\Users\user\.minikube\certs\ca.pem --> C:\Users\user\.minikube/ca.pem (1070 bytes)
I1204 16:27:25.862788    4912 exec_runner.go:144] found C:\Users\user\.minikube/cert.pem, removing ...
I1204 16:27:25.862788    4912 exec_runner.go:203] rm: C:\Users\user\.minikube\cert.pem
I1204 16:27:25.863311    4912 exec_runner.go:151] cp: C:\Users\user\.minikube\certs\cert.pem --> C:\Users\user\.minikube/cert.pem (1115 bytes)
I1204 16:27:25.886843    4912 exec_runner.go:144] found C:\Users\user\.minikube/key.pem, removing ...
I1204 16:27:25.886843    4912 exec_runner.go:203] rm: C:\Users\user\.minikube\key.pem
I1204 16:27:25.886843    4912 exec_runner.go:151] cp: C:\Users\user\.minikube\certs\key.pem --> C:\Users\user\.minikube/key.pem (1675 bytes)
I1204 16:27:25.888848    4912 provision.go:117] generating server cert: C:\Users\user\.minikube\machines\server.pem ca-key=C:\Users\user\.minikube\certs\ca.pem private-key=C:\Users\user\.minikube\certs\ca-key.pem org=user.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1204 16:27:26.329476    4912 provision.go:177] copyRemoteCerts
I1204 16:27:26.348358    4912 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1204 16:27:26.355949    4912 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1204 16:27:26.411341    4912 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:8526 SSHKeyPath:C:\Users\user\.minikube\machines\minikube\id_rsa Username:docker}
I1204 16:27:26.538118    4912 ssh_runner.go:362] scp C:\Users\user\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1070 bytes)
I1204 16:27:26.581607    4912 ssh_runner.go:362] scp C:\Users\user\.minikube\machines\server.pem --> /etc/docker/server.pem (1172 bytes)
I1204 16:27:26.622992    4912 ssh_runner.go:362] scp C:\Users\user\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1204 16:27:26.663911    4912 provision.go:87] duration metric: took 1.0212404s to configureAuth
I1204 16:27:26.663911    4912 ubuntu.go:206] setting minikube options for container-runtime
I1204 16:27:26.664444    4912 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1204 16:27:26.671271    4912 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1204 16:27:26.727029    4912 main.go:141] libmachine: Using SSH client type: native
I1204 16:27:26.727633    4912 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8617c0] 0x864300 <nil>  [] 0s} 127.0.0.1 8526 <nil> <nil>}
I1204 16:27:26.727633    4912 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1204 16:27:26.902875    4912 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1204 16:27:26.902875    4912 ubuntu.go:71] root file system type: overlay
I1204 16:27:26.903412    4912 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1204 16:27:26.910668    4912 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1204 16:27:26.967076    4912 main.go:141] libmachine: Using SSH client type: native
I1204 16:27:26.967421    4912 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8617c0] 0x864300 <nil>  [] 0s} 127.0.0.1 8526 <nil> <nil>}
I1204 16:27:26.967421    4912 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1204 16:27:27.180366    4912 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I1204 16:27:27.188782    4912 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1204 16:27:27.244428    4912 main.go:141] libmachine: Using SSH client type: native
I1204 16:27:27.244428    4912 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8617c0] 0x864300 <nil>  [] 0s} 127.0.0.1 8526 <nil> <nil>}
I1204 16:27:27.244428    4912 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1204 16:27:27.501465    4912 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1204 16:27:27.501465    4912 machine.go:96] duration metric: took 5.762199s to provisionDockerMachine
I1204 16:27:27.502496    4912 start.go:293] postStartSetup for "minikube" (driver="docker")
I1204 16:27:27.503470    4912 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1204 16:27:27.541805    4912 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1204 16:27:27.552639    4912 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1204 16:27:27.630583    4912 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:8526 SSHKeyPath:C:\Users\user\.minikube\machines\minikube\id_rsa Username:docker}
I1204 16:27:27.818146    4912 ssh_runner.go:195] Run: cat /etc/os-release
I1204 16:27:27.834873    4912 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1204 16:27:27.835431    4912 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1204 16:27:27.835431    4912 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1204 16:27:27.835431    4912 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I1204 16:27:27.835996    4912 filesync.go:126] Scanning C:\Users\user\.minikube\addons for local assets ...
I1204 16:27:27.836606    4912 filesync.go:126] Scanning C:\Users\user\.minikube\files for local assets ...
I1204 16:27:27.836606    4912 start.go:296] duration metric: took 334.1097ms for postStartSetup
I1204 16:27:27.869785    4912 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1204 16:27:27.907731    4912 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1204 16:27:27.965286    4912 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:8526 SSHKeyPath:C:\Users\user\.minikube\machines\minikube\id_rsa Username:docker}
I1204 16:27:28.142843    4912 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1204 16:27:28.168649    4912 fix.go:56] duration metric: took 7.4331108s for fixHost
I1204 16:27:28.168649    4912 start.go:83] releasing machines lock for "minikube", held for 7.4336164s
I1204 16:27:28.196967    4912 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1204 16:27:28.289847    4912 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I1204 16:27:28.298480    4912 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1204 16:27:28.303382    4912 ssh_runner.go:195] Run: cat /version.json
I1204 16:27:28.311685    4912 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1204 16:27:28.360480    4912 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:8526 SSHKeyPath:C:\Users\user\.minikube\machines\minikube\id_rsa Username:docker}
I1204 16:27:28.373820    4912 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:8526 SSHKeyPath:C:\Users\user\.minikube\machines\minikube\id_rsa Username:docker}
W1204 16:27:28.481450    4912 start.go:868] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I1204 16:27:28.513519    4912 ssh_runner.go:195] Run: systemctl --version
I1204 16:27:28.552321    4912 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1204 16:27:28.581280    4912 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W1204 16:27:28.600894    4912 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I1204 16:27:28.619269    4912 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1204 16:27:28.639945    4912 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1204 16:27:28.639945    4912 start.go:495] detecting cgroup driver to use...
I1204 16:27:28.639945    4912 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1204 16:27:28.641944    4912 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1204 16:27:28.704749    4912 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I1204 16:27:28.752553    4912 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1204 16:27:28.780306    4912 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I1204 16:27:28.803607    4912 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1204 16:27:28.852076    4912 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1204 16:27:28.895196    4912 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1204 16:27:28.939392    4912 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
W1204 16:27:28.956780    4912 out.go:285] ‚ùó  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W1204 16:27:28.957334    4912 out.go:285] üí°  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1204 16:27:28.980431    4912 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1204 16:27:29.016562    4912 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1204 16:27:29.047447    4912 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1204 16:27:29.078835    4912 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1204 16:27:29.114300    4912 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1204 16:27:29.148693    4912 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1204 16:27:29.181180    4912 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1204 16:27:29.294286    4912 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1204 16:27:29.436552    4912 start.go:495] detecting cgroup driver to use...
I1204 16:27:29.436552    4912 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1204 16:27:29.453601    4912 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1204 16:27:29.490704    4912 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1204 16:27:29.524082    4912 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I1204 16:27:29.619172    4912 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1204 16:27:29.711023    4912 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1204 16:27:29.769750    4912 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1204 16:27:29.848980    4912 ssh_runner.go:195] Run: which cri-dockerd
I1204 16:27:29.873900    4912 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1204 16:27:29.893046    4912 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I1204 16:27:29.939057    4912 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1204 16:27:30.052556    4912 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1204 16:27:30.138813    4912 docker.go:575] configuring docker to use "cgroupfs" as cgroup driver...
I1204 16:27:30.138813    4912 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1204 16:27:30.180395    4912 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I1204 16:27:30.213864    4912 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1204 16:27:30.314007    4912 ssh_runner.go:195] Run: sudo systemctl restart docker
I1204 16:27:33.484726    4912 ssh_runner.go:235] Completed: sudo systemctl restart docker: (3.1707192s)
I1204 16:27:33.510438    4912 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service docker
I1204 16:27:33.569711    4912 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1204 16:27:33.608088    4912 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I1204 16:27:33.641324    4912 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1204 16:27:33.675399    4912 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1204 16:27:33.794471    4912 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1204 16:27:33.975852    4912 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1204 16:27:34.089895    4912 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1204 16:27:34.173153    4912 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I1204 16:27:34.208380    4912 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1204 16:27:34.321767    4912 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1204 16:27:34.766521    4912 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1204 16:27:34.789088    4912 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1204 16:27:34.809905    4912 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1204 16:27:34.819349    4912 start.go:563] Will wait 60s for crictl version
I1204 16:27:34.845562    4912 ssh_runner.go:195] Run: which crictl
I1204 16:27:34.885006    4912 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1204 16:27:35.078750    4912 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.4.0
RuntimeApiVersion:  v1
I1204 16:27:35.087800    4912 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1204 16:27:35.239549    4912 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1204 16:27:35.274969    4912 out.go:252] üê≥  Preparing Kubernetes v1.34.0 on Docker 28.4.0 ...
I1204 16:27:35.282365    4912 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1204 16:27:35.500645    4912 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I1204 16:27:35.516760    4912 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I1204 16:27:35.524576    4912 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1204 16:27:35.551096    4912 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1204 16:27:35.609825    4912 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1204 16:27:35.611099    4912 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1204 16:27:35.618378    4912 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1204 16:27:35.658656    4912 docker.go:691] Got preloaded images: -- stdout --
grpc-calculator-server:latest
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1204 16:27:35.658656    4912 docker.go:621] Images already preloaded, skipping extraction
I1204 16:27:35.665596    4912 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1204 16:27:35.696520    4912 docker.go:691] Got preloaded images: -- stdout --
grpc-calculator-server:latest
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1204 16:27:35.697339    4912 cache_images.go:85] Images are preloaded, skipping loading
I1204 16:27:35.697925    4912 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.34.0 docker true true} ...
I1204 16:27:35.702114    4912 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1204 16:27:35.708965    4912 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1204 16:27:36.104956    4912 cni.go:84] Creating CNI manager for ""
I1204 16:27:36.104956    4912 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1204 16:27:36.106040    4912 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1204 16:27:36.106040    4912 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.34.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1204 16:27:36.106585    4912 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.34.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1204 16:27:36.123721    4912 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I1204 16:27:36.141923    4912 binaries.go:44] Found k8s binaries, skipping transfer
I1204 16:27:36.157784    4912 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1204 16:27:36.174198    4912 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1204 16:27:36.203119    4912 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1204 16:27:36.240034    4912 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2209 bytes)
I1204 16:27:36.292898    4912 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1204 16:27:36.301209    4912 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1204 16:27:36.341463    4912 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1204 16:27:36.447479    4912 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1204 16:27:36.519171    4912 certs.go:68] Setting up C:\Users\user\.minikube\profiles\minikube for IP: 192.168.49.2
I1204 16:27:36.519171    4912 certs.go:194] generating shared ca certs ...
I1204 16:27:36.519785    4912 certs.go:226] acquiring lock for ca certs: {Name:mk7164a99e354388efd17e38c9f3aba55425681b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1204 16:27:36.555926    4912 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\user\.minikube\ca.key
I1204 16:27:36.633994    4912 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\user\.minikube\proxy-client-ca.key
I1204 16:27:36.633994    4912 certs.go:256] generating profile certs ...
I1204 16:27:36.635060    4912 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\user\.minikube\profiles\minikube\client.key
I1204 16:27:36.663922    4912 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\user\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I1204 16:27:36.687866    4912 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\user\.minikube\profiles\minikube\proxy-client.key
I1204 16:27:36.692625    4912 certs.go:484] found cert: C:\Users\user\.minikube\certs\ca-key.pem (1679 bytes)
I1204 16:27:36.692625    4912 certs.go:484] found cert: C:\Users\user\.minikube\certs\ca.pem (1070 bytes)
I1204 16:27:36.692625    4912 certs.go:484] found cert: C:\Users\user\.minikube\certs\cert.pem (1115 bytes)
I1204 16:27:36.693147    4912 certs.go:484] found cert: C:\Users\user\.minikube\certs\key.pem (1675 bytes)
I1204 16:27:36.699571    4912 ssh_runner.go:362] scp C:\Users\user\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1204 16:27:36.759170    4912 ssh_runner.go:362] scp C:\Users\user\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1204 16:27:36.836025    4912 ssh_runner.go:362] scp C:\Users\user\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1204 16:27:36.960422    4912 ssh_runner.go:362] scp C:\Users\user\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1204 16:27:37.008499    4912 ssh_runner.go:362] scp C:\Users\user\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1204 16:27:37.066754    4912 ssh_runner.go:362] scp C:\Users\user\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1204 16:27:37.229899    4912 ssh_runner.go:362] scp C:\Users\user\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1204 16:27:37.544930    4912 ssh_runner.go:362] scp C:\Users\user\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1204 16:27:37.745921    4912 ssh_runner.go:362] scp C:\Users\user\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1204 16:27:38.039871    4912 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1204 16:27:38.159646    4912 ssh_runner.go:195] Run: openssl version
I1204 16:27:38.281001    4912 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1204 16:27:38.345836    4912 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1204 16:27:38.353839    4912 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Nov 24 21:48 /usr/share/ca-certificates/minikubeCA.pem
I1204 16:27:38.363552    4912 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1204 16:27:38.407785    4912 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1204 16:27:38.473590    4912 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1204 16:27:38.528750    4912 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1204 16:27:38.566897    4912 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1204 16:27:38.648775    4912 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1204 16:27:38.731917    4912 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1204 16:27:38.765479    4912 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1204 16:27:38.852491    4912 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1204 16:27:38.937122    4912 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1204 16:27:38.961382    4912 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1204 16:27:39.079189    4912 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1204 16:27:39.141115    4912 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I1204 16:27:39.141641    4912 kubeadm.go:589] restartPrimaryControlPlane start ...
I1204 16:27:39.165636    4912 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1204 16:27:39.323776    4912 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1204 16:27:39.351409    4912 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1204 16:27:39.454970    4912 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:12767"
I1204 16:27:39.455561    4912 kubeconfig.go:47] verify endpoint returned: got: 127.0.0.1:12767, want: 127.0.0.1:8530
I1204 16:27:39.463335    4912 kubeconfig.go:62] C:\Users\user\.kube\config needs updating (will repair): [kubeconfig needs server address update]
I1204 16:27:39.466319    4912 lock.go:35] WriteFile acquiring C:\Users\user\.kube\config: {Name:mkf0568b8c360d314f1ebed2036702ffdf9396af Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1204 16:27:39.571789    4912 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1204 16:27:39.822037    4912 kubeadm.go:626] The running cluster does not require reconfiguration: 127.0.0.1
I1204 16:27:39.822037    4912 kubeadm.go:593] duration metric: took 680.3962ms to restartPrimaryControlPlane
I1204 16:27:39.822037    4912 kubeadm.go:394] duration metric: took 884.9154ms to StartCluster
I1204 16:27:39.822037    4912 settings.go:142] acquiring lock: {Name:mkeb1fd696d13c02454084a505541301abc7d400 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1204 16:27:39.822723    4912 settings.go:150] Updating kubeconfig:  C:\Users\user\.kube\config
I1204 16:27:39.830625    4912 lock.go:35] WriteFile acquiring C:\Users\user\.kube\config: {Name:mkf0568b8c360d314f1ebed2036702ffdf9396af Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1204 16:27:39.836181    4912 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1204 16:27:39.842434    4912 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1204 16:27:39.843968    4912 out.go:179] üîé  Verifying Kubernetes components...
I1204 16:27:39.842763    4912 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1204 16:27:39.848249    4912 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1204 16:27:39.848249    4912 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1204 16:27:39.848847    4912 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1204 16:27:39.848847    4912 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W1204 16:27:39.848847    4912 addons.go:247] addon storage-provisioner should already be in state true
I1204 16:27:39.849668    4912 host.go:66] Checking if "minikube" exists ...
I1204 16:27:39.893003    4912 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1204 16:27:39.894011    4912 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1204 16:27:39.895592    4912 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1204 16:27:39.969365    4912 out.go:179]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1204 16:27:39.969904    4912 addons.go:238] Setting addon default-storageclass=true in "minikube"
W1204 16:27:39.969904    4912 addons.go:247] addon default-storageclass should already be in state true
I1204 16:27:39.969904    4912 host.go:66] Checking if "minikube" exists ...
I1204 16:27:39.970987    4912 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1204 16:27:39.970987    4912 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1204 16:27:39.978621    4912 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1204 16:27:39.986378    4912 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1204 16:27:40.054156    4912 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I1204 16:27:40.054156    4912 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1204 16:27:40.054156    4912 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:8526 SSHKeyPath:C:\Users\user\.minikube\machines\minikube\id_rsa Username:docker}
I1204 16:27:40.060543    4912 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1204 16:27:40.107820    4912 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:8526 SSHKeyPath:C:\Users\user\.minikube\machines\minikube\id_rsa Username:docker}
I1204 16:27:40.771333    4912 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1204 16:27:40.856578    4912 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1204 16:27:40.879684    4912 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1204 16:27:42.341305    4912 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.5699713s)
W1204 16:27:42.341305    4912 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1204 16:27:42.341822    4912 ssh_runner.go:235] Completed: sudo systemctl start kubelet: (1.4852437s)
I1204 16:27:42.341822    4912 retry.go:31] will retry after 196.358319ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1204 16:27:42.350452    4912 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.4707682s)
W1204 16:27:42.350452    4912 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1204 16:27:42.350452    4912 retry.go:31] will retry after 175.054042ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1204 16:27:42.354030    4912 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1204 16:27:42.409862    4912 api_server.go:52] waiting for apiserver process to appear ...
I1204 16:27:42.446584    4912 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1204 16:27:42.547401    4912 api_server.go:72] duration metric: took 2.7112198s to wait for apiserver process to appear ...
I1204 16:27:42.547401    4912 api_server.go:88] waiting for apiserver healthz status ...
I1204 16:27:42.550198    4912 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:8530/healthz ...
I1204 16:27:42.556357    4912 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1204 16:27:42.564785    4912 api_server.go:269] stopped: https://127.0.0.1:8530/healthz: Get "https://127.0.0.1:8530/healthz": EOF
I1204 16:27:42.565785    4912 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1204 16:27:43.047808    4912 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:8530/healthz ...
I1204 16:27:43.056430    4912 api_server.go:269] stopped: https://127.0.0.1:8530/healthz: Get "https://127.0.0.1:8530/healthz": EOF
I1204 16:27:43.548162    4912 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:8530/healthz ...
I1204 16:27:43.558418    4912 api_server.go:269] stopped: https://127.0.0.1:8530/healthz: Get "https://127.0.0.1:8530/healthz": EOF
I1204 16:27:43.645802    4912 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.0894454s)
W1204 16:27:43.646331    4912 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1204 16:27:43.646413    4912 retry.go:31] will retry after 386.442878ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1204 16:27:44.047932    4912 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:8530/healthz ...
I1204 16:27:44.099687    4912 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1204 16:27:49.049934    4912 api_server.go:269] stopped: https://127.0.0.1:8530/healthz: Get "https://127.0.0.1:8530/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1204 16:27:49.049934    4912 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:8530/healthz ...
I1204 16:27:50.046799    4912 api_server.go:279] https://127.0.0.1:8530/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1204 16:27:50.046799    4912 api_server.go:103] status: https://127.0.0.1:8530/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1204 16:27:50.046799    4912 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:8530/healthz ...
I1204 16:27:50.138522    4912 api_server.go:279] https://127.0.0.1:8530/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1204 16:27:50.138522    4912 api_server.go:103] status: https://127.0.0.1:8530/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1204 16:27:50.138522    4912 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:8530/healthz ...
I1204 16:27:50.239525    4912 api_server.go:279] https://127.0.0.1:8530/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\": RBAC: clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found","reason":"Forbidden","details":{},"code":403}
W1204 16:27:50.239525    4912 api_server.go:103] status: https://127.0.0.1:8530/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\": RBAC: clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found","reason":"Forbidden","details":{},"code":403}
I1204 16:27:50.547850    4912 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:8530/healthz ...
I1204 16:27:50.621548    4912 api_server.go:279] https://127.0.0.1:8530/healthz returned 500:
[+]ping ok
[+]log ok
[-]etcd failed: reason withheld
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1204 16:27:50.621548    4912 api_server.go:103] status: https://127.0.0.1:8530/healthz returned error 500:
[+]ping ok
[+]log ok
[-]etcd failed: reason withheld
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1204 16:27:51.048397    4912 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:8530/healthz ...
I1204 16:27:51.124923    4912 api_server.go:279] https://127.0.0.1:8530/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1204 16:27:51.125133    4912 api_server.go:103] status: https://127.0.0.1:8530/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1204 16:27:51.243028    4912 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (8.6772432s)
I1204 16:27:51.547923    4912 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:8530/healthz ...
I1204 16:27:51.623639    4912 api_server.go:279] https://127.0.0.1:8530/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1204 16:27:51.623639    4912 api_server.go:103] status: https://127.0.0.1:8530/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1204 16:27:52.048305    4912 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:8530/healthz ...
I1204 16:27:52.126951    4912 api_server.go:279] https://127.0.0.1:8530/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1204 16:27:52.126951    4912 api_server.go:103] status: https://127.0.0.1:8530/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1204 16:27:52.548623    4912 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:8530/healthz ...
I1204 16:27:52.814789    4912 api_server.go:279] https://127.0.0.1:8530/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1204 16:27:52.814789    4912 api_server.go:103] status: https://127.0.0.1:8530/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1204 16:27:53.048035    4912 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:8530/healthz ...
I1204 16:27:53.133154    4912 api_server.go:279] https://127.0.0.1:8530/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1204 16:27:53.133154    4912 api_server.go:103] status: https://127.0.0.1:8530/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1204 16:27:53.548317    4912 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:8530/healthz ...
I1204 16:27:53.628583    4912 api_server.go:279] https://127.0.0.1:8530/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1204 16:27:53.628583    4912 api_server.go:103] status: https://127.0.0.1:8530/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1204 16:27:54.048195    4912 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:8530/healthz ...
I1204 16:27:54.134268    4912 api_server.go:279] https://127.0.0.1:8530/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1204 16:27:54.134268    4912 api_server.go:103] status: https://127.0.0.1:8530/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1204 16:27:54.565190    4912 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:8530/healthz ...
I1204 16:27:54.822779    4912 api_server.go:279] https://127.0.0.1:8530/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1204 16:27:54.822779    4912 api_server.go:103] status: https://127.0.0.1:8530/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1204 16:27:55.048737    4912 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:8530/healthz ...
I1204 16:27:55.136855    4912 api_server.go:279] https://127.0.0.1:8530/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1204 16:27:55.136855    4912 api_server.go:103] status: https://127.0.0.1:8530/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1204 16:27:55.547751    4912 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:8530/healthz ...
I1204 16:27:55.725567    4912 api_server.go:279] https://127.0.0.1:8530/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1204 16:27:55.725567    4912 api_server.go:103] status: https://127.0.0.1:8530/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1204 16:27:56.048225    4912 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:8530/healthz ...
I1204 16:27:56.066137    4912 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (11.9664499s)
I1204 16:27:56.076879    4912 api_server.go:279] https://127.0.0.1:8530/healthz returned 200:
ok
I1204 16:27:56.106406    4912 out.go:179] üåü  Enabled addons: default-storageclass, storage-provisioner
I1204 16:27:56.109741    4912 addons.go:514] duration metric: took 16.2717021s for enable addons: enabled=[default-storageclass storage-provisioner]
I1204 16:27:56.114163    4912 api_server.go:141] control plane version: v1.34.0
I1204 16:27:56.114236    4912 api_server.go:131] duration metric: took 13.5668352s to wait for apiserver health ...
I1204 16:27:56.114574    4912 system_pods.go:43] waiting for kube-system pods to appear ...
I1204 16:27:56.157652    4912 system_pods.go:59] 8 kube-system pods found
I1204 16:27:56.157652    4912 system_pods.go:61] "coredns-66bc5c9577-2rgzn" [51384132-ebd2-4ad9-a09a-20b0f369fbf3] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1204 16:27:56.157652    4912 system_pods.go:61] "coredns-66bc5c9577-x4dqk" [39967ee3-d36f-4f25-a300-7d1638be10e3] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1204 16:27:56.157652    4912 system_pods.go:61] "etcd-minikube" [2bc1e977-96e6-4b86-89f4-4454cd92d096] Running
I1204 16:27:56.157652    4912 system_pods.go:61] "kube-apiserver-minikube" [40025001-b023-4635-b7df-01b518143b2d] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1204 16:27:56.157652    4912 system_pods.go:61] "kube-controller-manager-minikube" [a4faca45-f5b0-4446-a1bf-28dee9d68996] Running
I1204 16:27:56.157652    4912 system_pods.go:61] "kube-proxy-4xrn5" [d0809a99-b806-4c70-94eb-63267b71e1ac] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1204 16:27:56.157652    4912 system_pods.go:61] "kube-scheduler-minikube" [2e23b2da-9903-464e-9743-bda0cbf74174] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1204 16:27:56.157652    4912 system_pods.go:61] "storage-provisioner" [b4944b7c-73a2-4722-ba88-d99e38a8e1ba] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1204 16:27:56.157652    4912 system_pods.go:74] duration metric: took 43.0784ms to wait for pod list to return data ...
I1204 16:27:56.157652    4912 kubeadm.go:578] duration metric: took 16.3214708s to wait for: map[apiserver:true system_pods:true]
I1204 16:27:56.157652    4912 node_conditions.go:102] verifying NodePressure condition ...
I1204 16:27:56.184514    4912 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1204 16:27:56.184514    4912 node_conditions.go:123] node cpu capacity is 8
I1204 16:27:56.185717    4912 node_conditions.go:105] duration metric: took 28.0648ms to run NodePressure ...
I1204 16:27:56.185717    4912 start.go:241] waiting for startup goroutines ...
I1204 16:27:56.185717    4912 start.go:246] waiting for cluster config update ...
I1204 16:27:56.185717    4912 start.go:255] writing updated cluster config ...
I1204 16:27:56.242284    4912 ssh_runner.go:195] Run: rm -f paused
I1204 16:27:56.884479    4912 start.go:617] kubectl: 1.31.0, cluster: 1.34.0 (minor skew: 3)
I1204 16:27:56.886082    4912 out.go:203] 
W1204 16:27:56.887843    4912 out.go:285] ‚ùó  C:\Windows\system32\kubectl.exe is version 1.31.0, which may have incompatibilities with Kubernetes 1.34.0.
I1204 16:27:56.889016    4912 out.go:179]     ‚ñ™ Want kubectl v1.34.0? Try 'minikube kubectl -- get pods -A'
I1204 16:27:56.892538    4912 out.go:179] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Dec 04 14:27:33 minikube dockerd[815]: time="2025-12-04T14:27:33.218081305Z" level=warning msg="error locating sandbox id 5be4f78e7041a9b20723a14ca38ce0500f6d7d4b4d31ab9475b73b75eb80becb: sandbox 5be4f78e7041a9b20723a14ca38ce0500f6d7d4b4d31ab9475b73b75eb80becb not found"
Dec 04 14:27:33 minikube dockerd[815]: time="2025-12-04T14:27:33.218124809Z" level=warning msg="error locating sandbox id 90280d6eb4e3443fbac67de5813c89a7cba3375facce2a1970d11706933dc7f2: sandbox 90280d6eb4e3443fbac67de5813c89a7cba3375facce2a1970d11706933dc7f2 not found"
Dec 04 14:27:33 minikube dockerd[815]: time="2025-12-04T14:27:33.218164313Z" level=warning msg="error locating sandbox id ae49ebe63d2529a6e863af3b20843abd23d200bd78072df1395a8fc44c020e1f: sandbox ae49ebe63d2529a6e863af3b20843abd23d200bd78072df1395a8fc44c020e1f not found"
Dec 04 14:27:33 minikube dockerd[815]: time="2025-12-04T14:27:33.218270224Z" level=warning msg="error locating sandbox id 56b44140477e3bfd4f0e66c2dd8c2fcfd90733910dc24ed1055688afecc48072: sandbox 56b44140477e3bfd4f0e66c2dd8c2fcfd90733910dc24ed1055688afecc48072 not found"
Dec 04 14:27:33 minikube dockerd[815]: time="2025-12-04T14:27:33.218331430Z" level=warning msg="error locating sandbox id a623936e671a32d0248fd168ccbfd5cefb021c71bdde205e18b0448e8445b4e9: sandbox a623936e671a32d0248fd168ccbfd5cefb021c71bdde205e18b0448e8445b4e9 not found"
Dec 04 14:27:33 minikube dockerd[815]: time="2025-12-04T14:27:33.218461144Z" level=warning msg="error locating sandbox id cb3574d4bf2ab3b4976d750c5cdb3a885feb0e71bd142d9904b6b9bdea7a99a7: sandbox cb3574d4bf2ab3b4976d750c5cdb3a885feb0e71bd142d9904b6b9bdea7a99a7 not found"
Dec 04 14:27:33 minikube dockerd[815]: time="2025-12-04T14:27:33.218502648Z" level=warning msg="error locating sandbox id 92bd9860dec5362f8ba40b66f965fd463fe7391f57e8bdd21610d9080efe6c2c: sandbox 92bd9860dec5362f8ba40b66f965fd463fe7391f57e8bdd21610d9080efe6c2c not found"
Dec 04 14:27:33 minikube dockerd[815]: time="2025-12-04T14:27:33.218533951Z" level=warning msg="error locating sandbox id 76ece53042697c5b50d37f2537d9333106e5c83a982157571ebe74119a4bf9f6: sandbox 76ece53042697c5b50d37f2537d9333106e5c83a982157571ebe74119a4bf9f6 not found"
Dec 04 14:27:33 minikube dockerd[815]: time="2025-12-04T14:27:33.218574255Z" level=warning msg="error locating sandbox id 9ac9ae942a46c8146302b5e1da8701cfb5278bcbbf2cb8774186b8163e922d82: sandbox 9ac9ae942a46c8146302b5e1da8701cfb5278bcbbf2cb8774186b8163e922d82 not found"
Dec 04 14:27:33 minikube dockerd[815]: time="2025-12-04T14:27:33.218607559Z" level=warning msg="error locating sandbox id 176745f43811b6bfde1c41bbb12af0e7f398f0eabbdaeb7159775262fb6bf690: sandbox 176745f43811b6bfde1c41bbb12af0e7f398f0eabbdaeb7159775262fb6bf690 not found"
Dec 04 14:27:33 minikube dockerd[815]: time="2025-12-04T14:27:33.218638962Z" level=warning msg="error locating sandbox id cc099fc938981b62f9f13727d2627f68c17e78415fafa166b4f3ea7756fd18ff: sandbox cc099fc938981b62f9f13727d2627f68c17e78415fafa166b4f3ea7756fd18ff not found"
Dec 04 14:27:33 minikube dockerd[815]: time="2025-12-04T14:27:33.218672165Z" level=warning msg="error locating sandbox id 51877cddcfd995847e713758c58742b994d041b726213daafe6c59f961894fea: sandbox 51877cddcfd995847e713758c58742b994d041b726213daafe6c59f961894fea not found"
Dec 04 14:27:33 minikube dockerd[815]: time="2025-12-04T14:27:33.218705069Z" level=warning msg="error locating sandbox id 0c979af1e007a57d2f2063938c6f3c7c967965e40b3c4e96932e4ea1c45b095d: sandbox 0c979af1e007a57d2f2063938c6f3c7c967965e40b3c4e96932e4ea1c45b095d not found"
Dec 04 14:27:33 minikube dockerd[815]: time="2025-12-04T14:27:33.220336336Z" level=info msg="Loading containers: done."
Dec 04 14:27:33 minikube dockerd[815]: time="2025-12-04T14:27:33.275818624Z" level=info msg="Docker daemon" commit=249d679 containerd-snapshotter=false storage-driver=overlay2 version=28.4.0
Dec 04 14:27:33 minikube dockerd[815]: time="2025-12-04T14:27:33.276165160Z" level=info msg="Initializing buildkit"
Dec 04 14:27:33 minikube dockerd[815]: time="2025-12-04T14:27:33.470272461Z" level=info msg="Completed buildkit initialization"
Dec 04 14:27:33 minikube dockerd[815]: time="2025-12-04T14:27:33.480020661Z" level=info msg="Daemon has completed initialization"
Dec 04 14:27:33 minikube dockerd[815]: time="2025-12-04T14:27:33.480149474Z" level=info msg="API listen on /var/run/docker.sock"
Dec 04 14:27:33 minikube dockerd[815]: time="2025-12-04T14:27:33.480220881Z" level=info msg="API listen on /run/docker.sock"
Dec 04 14:27:33 minikube dockerd[815]: time="2025-12-04T14:27:33.480250685Z" level=info msg="API listen on [::]:2376"
Dec 04 14:27:33 minikube systemd[1]: Started Docker Application Container Engine.
Dec 04 14:27:34 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Dec 04 14:27:34 minikube cri-dockerd[1151]: time="2025-12-04T14:27:34Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Dec 04 14:27:34 minikube cri-dockerd[1151]: time="2025-12-04T14:27:34Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Dec 04 14:27:34 minikube cri-dockerd[1151]: time="2025-12-04T14:27:34Z" level=info msg="Start docker client with request timeout 0s"
Dec 04 14:27:34 minikube cri-dockerd[1151]: time="2025-12-04T14:27:34Z" level=info msg="Hairpin mode is set to hairpin-veth"
Dec 04 14:27:34 minikube cri-dockerd[1151]: time="2025-12-04T14:27:34Z" level=info msg="Loaded network plugin cni"
Dec 04 14:27:34 minikube cri-dockerd[1151]: time="2025-12-04T14:27:34Z" level=info msg="Docker cri networking managed by network plugin cni"
Dec 04 14:27:34 minikube cri-dockerd[1151]: time="2025-12-04T14:27:34Z" level=info msg="Setting cgroupDriver cgroupfs"
Dec 04 14:27:34 minikube cri-dockerd[1151]: time="2025-12-04T14:27:34Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Dec 04 14:27:34 minikube cri-dockerd[1151]: time="2025-12-04T14:27:34Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Dec 04 14:27:34 minikube cri-dockerd[1151]: time="2025-12-04T14:27:34Z" level=info msg="Start cri-dockerd grpc backend"
Dec 04 14:27:34 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Dec 04 14:27:37 minikube cri-dockerd[1151]: time="2025-12-04T14:27:37Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"grpc-calculator-server-7dc47f9c64-vvmd7_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"c713bc1aaecec336f98140155f5a2c9d1eb9c9e3d8be98c6bcd7f770eded083b\""
Dec 04 14:27:37 minikube cri-dockerd[1151]: time="2025-12-04T14:27:37Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-66bc5c9577-x4dqk_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"3f4fc30c26415b6aec399bbba5f675e3fc8d8c998a95905937b8a12f583e4997\""
Dec 04 14:27:37 minikube cri-dockerd[1151]: time="2025-12-04T14:27:37Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-66bc5c9577-x4dqk_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"00d9b2994ea659434371d3c2638343a1de1c33f225fa4fc9956b4bcdf395db89\""
Dec 04 14:27:37 minikube cri-dockerd[1151]: time="2025-12-04T14:27:37Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-66bc5c9577-2rgzn_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"71be1b018d530200184d53635b7749fdba80659c837f83bacc33ac121f691d90\""
Dec 04 14:27:37 minikube cri-dockerd[1151]: time="2025-12-04T14:27:37Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-66bc5c9577-2rgzn_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"5df8b65d33fd6f24aea33b70c3ef1c1cce642bb33b7a231e8d58cbc67528de6c\""
Dec 04 14:27:39 minikube cri-dockerd[1151]: time="2025-12-04T14:27:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/37536a7859b082d71ec30d9d9c9a0b37ad03ce23646159463958ff1857e26011/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 04 14:27:39 minikube cri-dockerd[1151]: time="2025-12-04T14:27:39Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-66bc5c9577-x4dqk_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"3f4fc30c26415b6aec399bbba5f675e3fc8d8c998a95905937b8a12f583e4997\""
Dec 04 14:27:39 minikube cri-dockerd[1151]: time="2025-12-04T14:27:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/722cd5175440db1737bb2cd67ed0c401052cc4c9e85545b619fee452831c8a64/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 04 14:27:39 minikube cri-dockerd[1151]: time="2025-12-04T14:27:39Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-66bc5c9577-2rgzn_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"71be1b018d530200184d53635b7749fdba80659c837f83bacc33ac121f691d90\""
Dec 04 14:27:39 minikube cri-dockerd[1151]: time="2025-12-04T14:27:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/88491dd36b0e006b337535657ee877a8dc4d97fdebde2311e195d9c9a4a7dc77/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 04 14:27:39 minikube cri-dockerd[1151]: time="2025-12-04T14:27:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/702117f2623583120aca15ca88161c73419f71abf851fd19fbe42eec58d36304/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 04 14:27:50 minikube cri-dockerd[1151]: time="2025-12-04T14:27:50Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Dec 04 14:27:53 minikube cri-dockerd[1151]: time="2025-12-04T14:27:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c62c989c1e0c46160be737ec381713d48d7d2e11cfbf41b674c0a8861652066a/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 04 14:27:53 minikube cri-dockerd[1151]: time="2025-12-04T14:27:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d10ec2965fc80e46a08e9c5728484c6f87fa7c24be776681a41a17b9e4d9c858/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 04 14:27:53 minikube cri-dockerd[1151]: time="2025-12-04T14:27:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2347e858cead5d9cc487869d33177f9513edf91a31ffaf37844c6914afcc2bb9/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 04 14:27:53 minikube cri-dockerd[1151]: time="2025-12-04T14:27:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b21df9355256b8a9f8f464133c0b4192168cb2eaf567a4db3a60422c422250dd/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 04 14:27:54 minikube cri-dockerd[1151]: time="2025-12-04T14:27:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5c018bb65dab7b47ad1585fbf5e9b2dc53685ba6ba968716cedd70fc8234fb89/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 04 14:27:57 minikube dockerd[815]: time="2025-12-04T14:27:57.340412535Z" level=info msg="ignoring event" container=c31a980d3d090dd3ac01ba14a738b0147628e02962a7ca5e07b8b20c17c1c00c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 04 14:28:22 minikube cri-dockerd[1151]: time="2025-12-04T14:28:22Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4fa62a3f33be18b825d0f72d262454701ba7e63e59f7f9e478c4682cdb571007/resolv.conf as [nameserver 10.96.0.10 search recipes-app.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 04 14:28:23 minikube cri-dockerd[1151]: time="2025-12-04T14:28:23Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ac32262af69ceeb299c3e4d325fc881404a19776a30a611b64cbf4e4250ffa4f/resolv.conf as [nameserver 10.96.0.10 search recipes-app.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 04 14:28:23 minikube cri-dockerd[1151]: time="2025-12-04T14:28:23Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7758bb2d511ac777b281bc7083114ba56e5590f9fdfeea8a6dd01c9e44583c68/resolv.conf as [nameserver 10.96.0.10 search recipes-app.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 04 14:28:34 minikube cri-dockerd[1151]: time="2025-12-04T14:28:34Z" level=info msg="Pulling image postgres:16: 2b3e79aa53c6: Downloading [====>                                              ]  10.78MB/113.1MB"
Dec 04 14:28:44 minikube cri-dockerd[1151]: time="2025-12-04T14:28:44Z" level=info msg="Pulling image postgres:16: 2b3e79aa53c6: Downloading [=====================>                             ]  49.54MB/113.1MB"
Dec 04 14:28:54 minikube cri-dockerd[1151]: time="2025-12-04T14:28:54Z" level=info msg="Pulling image postgres:16: 2b3e79aa53c6: Downloading [=======================================>           ]  90.42MB/113.1MB"
Dec 04 14:29:04 minikube cri-dockerd[1151]: time="2025-12-04T14:29:04Z" level=info msg="Pulling image postgres:16: 2b3e79aa53c6: Extracting [==========================>                        ]  59.05MB/113.1MB"
Dec 04 14:29:12 minikube cri-dockerd[1151]: time="2025-12-04T14:29:12Z" level=info msg="Stop pulling image postgres:16: Status: Downloaded newer image for postgres:16"


==> container status <==
CONTAINER           IMAGE                                                                              CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
e19b620c4b7d4       postgres@sha256:cf2a05fe40887b721e4b3dbac8fd32673c08292dcc8ba6b62b52b7f640433bd0   49 seconds ago       Running             postgres                  0                   4fa62a3f33be1       recipes-db-7467f7949b-jf2qx
452c084c53b3a       6e38f40d628db                                                                      About a minute ago   Running             storage-provisioner       6                   d10ec2965fc80       storage-provisioner
06eb247e9fd9a       52546a367cc9e                                                                      2 minutes ago        Running             coredns                   2                   5c018bb65dab7       coredns-66bc5c9577-2rgzn
7c358220538c2       52546a367cc9e                                                                      2 minutes ago        Running             coredns                   2                   2347e858cead5       coredns-66bc5c9577-x4dqk
4a6d0290da8c3       208de118b36f0                                                                      2 minutes ago        Running             grpc-server               1                   b21df9355256b       grpc-calculator-server-7dc47f9c64-vvmd7
37a8f50d6d3a3       df0860106674d                                                                      2 minutes ago        Running             kube-proxy                2                   c62c989c1e0c4       kube-proxy-4xrn5
c31a980d3d090       6e38f40d628db                                                                      2 minutes ago        Exited              storage-provisioner       5                   d10ec2965fc80       storage-provisioner
cc6bb9159b2ec       90550c43ad2bc                                                                      2 minutes ago        Running             kube-apiserver            2                   37536a7859b08       kube-apiserver-minikube
9266bab6840a0       5f1f5298c888d                                                                      2 minutes ago        Running             etcd                      2                   88491dd36b0e0       etcd-minikube
b11f758168604       46169d968e920                                                                      2 minutes ago        Running             kube-scheduler            2                   702117f262358       kube-scheduler-minikube
f0bb3fac5c580       a0af72f2ec6d6                                                                      2 minutes ago        Running             kube-controller-manager   2                   722cd5175440d       kube-controller-manager-minikube
be2b3f43d45b1       208de118b36f0                                                                      6 days ago           Exited              grpc-server               0                   c713bc1aaecec       grpc-calculator-server-7dc47f9c64-vvmd7
4d3253a32b9a8       52546a367cc9e                                                                      6 days ago           Exited              coredns                   1                   71be1b018d530       coredns-66bc5c9577-2rgzn
5499802b3c5c9       52546a367cc9e                                                                      6 days ago           Exited              coredns                   1                   3f4fc30c26415       coredns-66bc5c9577-x4dqk
df243a4dfa8cd       df0860106674d                                                                      6 days ago           Exited              kube-proxy                1                   133a88cb5e698       kube-proxy-4xrn5
abdcc98d3cf31       90550c43ad2bc                                                                      6 days ago           Exited              kube-apiserver            1                   da2e78984781c       kube-apiserver-minikube
61fcd45eaa15f       a0af72f2ec6d6                                                                      6 days ago           Exited              kube-controller-manager   1                   e464bd19fd016       kube-controller-manager-minikube
d2e776251cf7b       46169d968e920                                                                      6 days ago           Exited              kube-scheduler            1                   86261cde2119e       kube-scheduler-minikube
9be3e1c293f9f       5f1f5298c888d                                                                      6 days ago           Exited              etcd                      1                   5829a8f11e128       etcd-minikube


==> coredns [06eb247e9fd9] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1


==> coredns [4d3253a32b9a] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1


==> coredns [5499802b3c5c] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1


==> coredns [7c358220538c] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_11_24T23_48_22_0700
                    minikube.k8s.io/version=v1.37.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 24 Nov 2025 21:48:17 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 04 Dec 2025 14:29:52 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 04 Dec 2025 14:29:43 +0000   Mon, 24 Nov 2025 21:48:12 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 04 Dec 2025 14:29:43 +0000   Mon, 24 Nov 2025 21:48:12 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 04 Dec 2025 14:29:43 +0000   Mon, 24 Nov 2025 21:48:12 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 04 Dec 2025 14:29:43 +0000   Mon, 24 Nov 2025 21:48:17 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             5994720Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             5994720Ki
  pods:               110
System Info:
  Machine ID:                 2d7dfa0de0dc40158461e53123e89af5
  System UUID:                2d7dfa0de0dc40158461e53123e89af5
  Boot ID:                    a3f50349-3679-4c9a-a44e-1052542a80e5
  Kernel Version:             6.6.87.2-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.4.0
  Kubelet Version:            v1.34.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                       ------------  ----------  ---------------  -------------  ---
  default                     grpc-calculator-server-7dc47f9c64-vvmd7    0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d15h
  kube-system                 coredns-66bc5c9577-2rgzn                   100m (1%)     0 (0%)      70Mi (1%)        170Mi (2%)     9d
  kube-system                 coredns-66bc5c9577-x4dqk                   100m (1%)     0 (0%)      70Mi (1%)        170Mi (2%)     9d
  kube-system                 etcd-minikube                              100m (1%)     0 (0%)      100Mi (1%)       0 (0%)         9d
  kube-system                 kube-apiserver-minikube                    250m (3%)     0 (0%)      0 (0%)           0 (0%)         9d
  kube-system                 kube-controller-manager-minikube           200m (2%)     0 (0%)      0 (0%)           0 (0%)         9d
  kube-system                 kube-proxy-4xrn5                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         9d
  kube-system                 kube-scheduler-minikube                    100m (1%)     0 (0%)      0 (0%)           0 (0%)         9d
  kube-system                 storage-provisioner                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         9d
  recipes-app                 recipes-backend-5447f78f68-95mgh           0 (0%)        0 (0%)      0 (0%)           0 (0%)         99s
  recipes-app                 recipes-db-7467f7949b-jf2qx                0 (0%)        0 (0%)      0 (0%)           0 (0%)         100s
  recipes-app                 recipes-frontend-d7c9dd5b9-b9cj2           0 (0%)        0 (0%)      0 (0%)           0 (0%)         99s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (10%)  0 (0%)
  memory             240Mi (4%)  340Mi (5%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                   Age                    From             Message
  ----     ------                   ----                   ----             -------
  Normal   Starting                 2m2s                   kube-proxy       
  Normal   Starting                 2m24s                  kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory  2m24s (x8 over 2m24s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    2m24s (x8 over 2m24s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     2m24s (x7 over 2m24s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  2m24s                  kubelet          Updated Node Allocatable limit across pods
  Warning  Rebooted                 2m11s                  kubelet          Node minikube has been rebooted, boot id: a3f50349-3679-4c9a-a44e-1052542a80e5
  Normal   RegisteredNode           2m3s                   node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Dec 4 14:13] MMIO Stale Data CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/processor_mmio_stale_data.html for more details.
[  +0.032908] PCI: Fatal: No config space access function found
[  +0.064196] PCI: System does not support PCI
[  +0.163003] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +1.830927] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2058: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.020857] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Beirut not found. Is the tzdata package installed?
[  +0.280327] pulseaudio[265]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[  +0.241233] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.004489] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000787] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000833] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000950] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.003181] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001036] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001287] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001837] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001182] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +1.918999] netlink: 'initd': attribute type 4 has an invalid length.
[  +0.143969] virtiofs: Unknown parameter 'negative_dentry_timeout'
[  +0.967996] WSL (216) ERROR: CheckConnection: getaddrinfo() failed: -5
[Dec 4 14:16] hrtimer: interrupt took 1558610 ns


==> etcd [9266bab6840a] <==
{"level":"warn","ts":"2025-12-04T14:27:47.917943Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35068","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:47.925296Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35098","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:47.935017Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35114","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:47.943921Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35146","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:47.951657Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35170","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:47.960575Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35188","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:47.967943Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35202","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:47.976184Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35214","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:47.984912Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35230","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.061599Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35242","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.071986Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35274","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.080414Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35282","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.095791Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35288","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.105465Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35304","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.116764Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35318","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.128631Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35322","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.139542Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35348","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.148770Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35360","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.158512Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35390","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.168012Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35398","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.177099Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35420","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.184906Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35430","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.204456Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35450","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.217871Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35454","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.227520Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35474","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.239410Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35480","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.248900Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35500","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.272944Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35542","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.284567Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35560","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.315177Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35572","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.335506Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35584","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.372444Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35604","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.403362Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35632","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.421306Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35646","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.456485Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35676","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.475034Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35706","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.488433Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35714","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.516698Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35736","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.536322Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35752","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.550635Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35762","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.567226Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35780","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.624397Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35794","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.652911Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35812","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.661756Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35838","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.671787Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35852","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-04T14:27:48.759833Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:35872","server-name":"","error":"EOF"}
{"level":"info","ts":"2025-12-04T14:27:50.339067Z","caller":"traceutil/trace.go:172","msg":"trace[1356767012] transaction","detail":"{read_only:false; number_of_response:0; response_revision:10232; }","duration":"101.633542ms","start":"2025-12-04T14:27:50.237401Z","end":"2025-12-04T14:27:50.339034Z","steps":["trace[1356767012] 'process raft request'  (duration: 101.393418ms)"],"step_count":1}
{"level":"info","ts":"2025-12-04T14:27:50.339877Z","caller":"traceutil/trace.go:172","msg":"trace[1975619816] transaction","detail":"{read_only:false; response_revision:10232; number_of_response:1; }","duration":"102.528333ms","start":"2025-12-04T14:27:50.237330Z","end":"2025-12-04T14:27:50.339859Z","steps":["trace[1975619816] 'process raft request'  (duration: 91.541415ms)"],"step_count":1}
{"level":"info","ts":"2025-12-04T14:27:50.340161Z","caller":"traceutil/trace.go:172","msg":"trace[131054061] transaction","detail":"{read_only:false; number_of_response:1; response_revision:10233; }","duration":"100.073383ms","start":"2025-12-04T14:27:50.240061Z","end":"2025-12-04T14:27:50.340135Z","steps":["trace[131054061] 'process raft request'  (duration: 98.820156ms)"],"step_count":1}
{"level":"warn","ts":"2025-12-04T14:27:50.618478Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"102.325112ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/minions/minikube\" limit:1 ","response":"range_response_count:1 size:4487"}
{"level":"info","ts":"2025-12-04T14:27:50.618629Z","caller":"traceutil/trace.go:172","msg":"trace[1011741899] range","detail":"{range_begin:/registry/minions/minikube; range_end:; response_count:1; response_revision:10233; }","duration":"102.535634ms","start":"2025-12-04T14:27:50.516067Z","end":"2025-12-04T14:27:50.618602Z","steps":["trace[1011741899] 'range keys from in-memory index tree'  (duration: 102.151994ms)"],"step_count":1}
{"level":"warn","ts":"2025-12-04T14:27:50.618423Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"103.292311ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-12-04T14:27:50.618911Z","caller":"traceutil/trace.go:172","msg":"trace[720313778] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:10233; }","duration":"103.820364ms","start":"2025-12-04T14:27:50.515078Z","end":"2025-12-04T14:27:50.618898Z","steps":["trace[720313778] 'range keys from in-memory index tree'  (duration: 103.180499ms)"],"step_count":1}
{"level":"info","ts":"2025-12-04T14:27:50.638002Z","caller":"traceutil/trace.go:172","msg":"trace[1510829955] transaction","detail":"{read_only:false; response_revision:10234; number_of_response:1; }","duration":"109.683961ms","start":"2025-12-04T14:27:50.528239Z","end":"2025-12-04T14:27:50.637923Z","steps":["trace[1510829955] 'process raft request'  (duration: 108.176208ms)"],"step_count":1}
{"level":"info","ts":"2025-12-04T14:27:50.819400Z","caller":"traceutil/trace.go:172","msg":"trace[903761926] transaction","detail":"{read_only:false; number_of_response:0; response_revision:10235; }","duration":"105.54464ms","start":"2025-12-04T14:27:50.713777Z","end":"2025-12-04T14:27:50.819322Z","steps":["trace[903761926] 'process raft request'  (duration: 105.314916ms)"],"step_count":1}
{"level":"warn","ts":"2025-12-04T14:27:51.519630Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"100.761953ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/system:public-info-viewer\" limit:1 ","response":"range_response_count:1 size:613"}
{"level":"info","ts":"2025-12-04T14:27:51.519767Z","caller":"traceutil/trace.go:172","msg":"trace[524656673] range","detail":"{range_begin:/registry/clusterroles/system:public-info-viewer; range_end:; response_count:1; response_revision:10245; }","duration":"100.938571ms","start":"2025-12-04T14:27:51.418806Z","end":"2025-12-04T14:27:51.519745Z","steps":["trace[524656673] 'agreement among raft nodes before linearized reading'  (duration: 100.551632ms)"],"step_count":1}
{"level":"warn","ts":"2025-12-04T14:27:54.537720Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"101.473126ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterrolebindings/system:controller:namespace-controller\" limit:1 ","response":"range_response_count:1 size:755"}
{"level":"info","ts":"2025-12-04T14:27:54.537855Z","caller":"traceutil/trace.go:172","msg":"trace[500813409] range","detail":"{range_begin:/registry/clusterrolebindings/system:controller:namespace-controller; range_end:; response_count:1; response_revision:10290; }","duration":"101.660044ms","start":"2025-12-04T14:27:54.436163Z","end":"2025-12-04T14:27:54.537824Z","steps":["trace[500813409] 'range keys from in-memory index tree'  (duration: 93.024165ms)"],"step_count":1}
{"level":"info","ts":"2025-12-04T14:27:54.550060Z","caller":"traceutil/trace.go:172","msg":"trace[1220528234] transaction","detail":"{read_only:false; response_revision:10292; number_of_response:1; }","duration":"112.518549ms","start":"2025-12-04T14:27:54.437511Z","end":"2025-12-04T14:27:54.550029Z","steps":["trace[1220528234] 'process raft request'  (duration: 112.291126ms)"],"step_count":1}


==> etcd [9be3e1c293f9] <==
{"level":"info","ts":"2025-11-28T00:24:39.129642Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":5442}
{"level":"info","ts":"2025-11-28T00:24:39.137563Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":5442,"took":"7.343872ms","hash":564695471,"current-db-size-bytes":2260992,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":1413120,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-11-28T00:24:39.137737Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":564695471,"revision":5442,"compact-revision":5204}
{"level":"info","ts":"2025-11-28T00:29:39.098590Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":5680}
{"level":"info","ts":"2025-11-28T00:29:39.107738Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":5680,"took":"8.789297ms","hash":2048900662,"current-db-size-bytes":2260992,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":1413120,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-11-28T00:29:39.107810Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2048900662,"revision":5680,"compact-revision":5442}
{"level":"info","ts":"2025-11-28T00:34:39.064655Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":5919}
{"level":"info","ts":"2025-11-28T00:34:39.072380Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":5919,"took":"7.193386ms","hash":511038507,"current-db-size-bytes":2260992,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":1421312,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-11-28T00:34:39.072509Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":511038507,"revision":5919,"compact-revision":5680}
{"level":"info","ts":"2025-11-28T00:39:39.027552Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":6157}
{"level":"info","ts":"2025-11-28T00:39:39.038569Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":6157,"took":"9.609746ms","hash":4088451983,"current-db-size-bytes":2260992,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":1429504,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-11-28T00:39:39.038775Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":4088451983,"revision":6157,"compact-revision":5919}
{"level":"info","ts":"2025-11-28T00:44:39.012101Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":6395}
{"level":"info","ts":"2025-11-28T00:44:39.038392Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":6395,"took":"24.658135ms","hash":2569080461,"current-db-size-bytes":2260992,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":1433600,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-11-28T00:44:39.038968Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2569080461,"revision":6395,"compact-revision":6157}
{"level":"info","ts":"2025-11-28T00:49:38.996831Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":6634}
{"level":"info","ts":"2025-11-28T00:49:39.007031Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":6634,"took":"9.567395ms","hash":3314210953,"current-db-size-bytes":2260992,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":1433600,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-11-28T00:49:39.007164Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3314210953,"revision":6634,"compact-revision":6395}
{"level":"info","ts":"2025-11-28T00:54:38.984049Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":6873}
{"level":"info","ts":"2025-11-28T00:54:38.990730Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":6873,"took":"6.274941ms","hash":1527418405,"current-db-size-bytes":2260992,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":1437696,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-11-28T00:54:38.990808Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1527418405,"revision":6873,"compact-revision":6634}
{"level":"info","ts":"2025-11-28T00:59:38.949362Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":7111}
{"level":"info","ts":"2025-11-28T00:59:38.959995Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":7111,"took":"9.890508ms","hash":2575196032,"current-db-size-bytes":2260992,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":1433600,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-11-28T00:59:38.960152Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2575196032,"revision":7111,"compact-revision":6873}
{"level":"info","ts":"2025-11-28T01:04:38.924158Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":7349}
{"level":"info","ts":"2025-11-28T01:04:38.937618Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":7349,"took":"10.814495ms","hash":1424755288,"current-db-size-bytes":2260992,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":1433600,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-11-28T01:04:38.937909Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1424755288,"revision":7349,"compact-revision":7111}
{"level":"info","ts":"2025-11-28T01:09:38.907222Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":7588}
{"level":"info","ts":"2025-11-28T01:09:38.920159Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":7588,"took":"12.066893ms","hash":3526873312,"current-db-size-bytes":2260992,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":1449984,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-11-28T01:09:38.920406Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3526873312,"revision":7588,"compact-revision":7349}
{"level":"info","ts":"2025-11-28T01:14:38.874984Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":7827}
{"level":"info","ts":"2025-11-28T01:14:38.878538Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":7827,"took":"3.297245ms","hash":2764286280,"current-db-size-bytes":2260992,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":1449984,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-11-28T01:14:38.878592Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2764286280,"revision":7827,"compact-revision":7588}
{"level":"info","ts":"2025-11-28T01:14:49.217303Z","caller":"etcdserver/server.go:2185","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":10001,"local-member-snapshot-index":0,"local-member-snapshot-count":10000,"snapshot-forced":false}
{"level":"info","ts":"2025-11-28T01:14:49.226157Z","caller":"etcdserver/server.go:2230","msg":"saved snapshot to disk","snapshot-index":10001}
{"level":"info","ts":"2025-11-28T01:19:38.851014Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":8065}
{"level":"info","ts":"2025-11-28T01:19:38.859973Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":8065,"took":"8.367485ms","hash":415212994,"current-db-size-bytes":2260992,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":1433600,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-11-28T01:19:38.860147Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":415212994,"revision":8065,"compact-revision":7827}
{"level":"info","ts":"2025-11-28T01:24:38.835149Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":8304}
{"level":"info","ts":"2025-11-28T01:24:38.848282Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":8304,"took":"12.00759ms","hash":1437269090,"current-db-size-bytes":2260992,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":1421312,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-11-28T01:24:38.848452Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1437269090,"revision":8304,"compact-revision":8065}
{"level":"info","ts":"2025-11-28T01:29:38.822057Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":8543}
{"level":"info","ts":"2025-11-28T01:29:38.840665Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":8543,"took":"17.456575ms","hash":3208333524,"current-db-size-bytes":2260992,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":1413120,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-11-28T01:29:38.840949Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3208333524,"revision":8543,"compact-revision":8304}
{"level":"info","ts":"2025-11-28T01:34:38.791002Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":8780}
{"level":"info","ts":"2025-11-28T01:34:38.800499Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":8780,"took":"8.979344ms","hash":2649286020,"current-db-size-bytes":2260992,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":1421312,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-11-28T01:34:38.800657Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2649286020,"revision":8780,"compact-revision":8543}
{"level":"warn","ts":"2025-11-28T01:38:29.191172Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"112.454905ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128041609234806482 > lease_revoke:<id:70cc9ac78608f684>","response":"size:29"}
{"level":"info","ts":"2025-11-28T01:39:38.773924Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":9020}
{"level":"info","ts":"2025-11-28T01:39:38.787136Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":9020,"took":"12.726846ms","hash":3643615073,"current-db-size-bytes":2260992,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":1433600,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-11-28T01:39:38.787274Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3643615073,"revision":9020,"compact-revision":8780}
{"level":"info","ts":"2025-11-28T01:44:38.760732Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":9258}
{"level":"info","ts":"2025-11-28T01:44:38.780326Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":9258,"took":"18.210552ms","hash":2349366462,"current-db-size-bytes":2260992,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":1404928,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-11-28T01:44:38.780571Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2349366462,"revision":9258,"compact-revision":9020}
{"level":"info","ts":"2025-11-28T01:49:38.748752Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":9495}
{"level":"info","ts":"2025-11-28T01:49:38.753369Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":9495,"took":"4.372144ms","hash":1227584413,"current-db-size-bytes":2260992,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":1376256,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-11-28T01:49:38.753464Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1227584413,"revision":9495,"compact-revision":9258}
{"level":"info","ts":"2025-11-28T01:54:38.766160Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":9733}
{"level":"info","ts":"2025-11-28T01:54:38.788331Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":9733,"took":"20.872741ms","hash":3688234641,"current-db-size-bytes":2260992,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":1380352,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-11-28T01:54:38.788779Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3688234641,"revision":9733,"compact-revision":9495}


==> kernel <==
 14:30:01 up 16 min,  0 users,  load average: 1.33, 1.25, 0.83
Linux minikube 6.6.87.2-microsoft-standard-WSL2 #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [abdcc98d3cf3] <==
I1128 01:24:41.718022       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1128 01:24:49.088924       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:25:42.045534       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:25:57.246176       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:26:59.839102       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:27:07.427098       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:28:00.421172       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:28:29.325214       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:29:29.566460       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:29:30.321643       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:30:38.075564       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:30:52.414328       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:32:02.526847       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:32:21.518981       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:33:30.162963       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:33:32.989037       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:34:41.623067       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1128 01:34:41.757815       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:34:50.798494       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:35:54.741365       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:36:09.573094       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:37:11.261104       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:37:16.122075       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:38:17.714172       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:38:24.551550       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:39:35.024140       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:39:50.739633       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:40:44.195429       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:41:14.561802       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:42:09.753820       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:42:15.029912       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:43:22.847092       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:43:36.790003       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:44:23.342198       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:44:41.543103       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1128 01:44:49.185899       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:45:47.604586       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:46:10.851938       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:46:50.510161       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:47:32.994282       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:48:06.802659       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:48:40.711515       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:49:21.329588       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:49:52.405616       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:50:49.586064       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:51:12.643799       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:52:09.264243       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:52:40.203314       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:53:09.315848       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:53:55.995136       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:54:38.548710       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:54:41.475856       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1128 01:55:05.437061       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:55:59.115188       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:56:30.493812       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:57:09.621713       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:57:54.693953       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:58:27.833154       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:59:02.067891       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1128 01:59:33.831561       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-apiserver [cc6bb9159b2e] <==
I1204 14:27:49.909320       1 remote_available_controller.go:425] Starting RemoteAvailability controller
I1204 14:27:49.909329       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1204 14:27:49.909359       1 aggregator.go:169] waiting for initial CRD sync...
I1204 14:27:49.910386       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I1204 14:27:49.910496       1 cluster_authentication_trust_controller.go:459] Starting cluster_authentication_trust_controller controller
I1204 14:27:49.910522       1 shared_informer.go:349] "Waiting for caches to sync" controller="cluster_authentication_trust_controller"
I1204 14:27:49.910656       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1204 14:27:49.910921       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1204 14:27:49.929763       1 controller.go:119] Starting legacy_token_tracking_controller
I1204 14:27:49.929846       1 shared_informer.go:349] "Waiting for caches to sync" controller="configmaps"
I1204 14:27:49.934533       1 repairip.go:210] Starting ipallocator-repair-controller
I1204 14:27:49.934953       1 shared_informer.go:349] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I1204 14:27:49.944222       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1204 14:27:49.949873       1 controller.go:142] Starting OpenAPI controller
I1204 14:27:49.951482       1 establishing_controller.go:81] Starting EstablishingController
I1204 14:27:50.011693       1 controller.go:90] Starting OpenAPI V3 controller
I1204 14:27:50.011963       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1204 14:27:50.012043       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1204 14:27:50.012067       1 crd_finalizer.go:269] Starting CRDFinalizer
I1204 14:27:50.012092       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1204 14:27:50.012119       1 shared_informer.go:349] "Waiting for caches to sync" controller="crd-autoregister"
I1204 14:27:50.012466       1 default_servicecidr_controller.go:111] Starting kubernetes-service-cidr-controller
I1204 14:27:50.012577       1 shared_informer.go:349] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I1204 14:27:50.026616       1 naming_controller.go:299] Starting NamingConditionController
I1204 14:27:50.131614       1 shared_informer.go:356] "Caches are synced" controller="configmaps"
I1204 14:27:50.132893       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I1204 14:27:50.212073       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1204 14:27:50.221877       1 shared_informer.go:356] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I1204 14:27:50.222151       1 policy_source.go:240] refreshing policies
I1204 14:27:50.221910       1 shared_informer.go:356] "Caches are synced" controller="crd-autoregister"
I1204 14:27:50.230289       1 aggregator.go:171] initial CRD sync complete...
I1204 14:27:50.230388       1 autoregister_controller.go:144] Starting autoregister controller
I1204 14:27:50.230551       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1204 14:27:50.230565       1 cache.go:39] Caches are synced for autoregister controller
I1204 14:27:50.231270       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I1204 14:27:50.232914       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1204 14:27:50.232941       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1204 14:27:50.233507       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1204 14:27:50.233738       1 cache.go:39] Caches are synced for LocalAvailability controller
I1204 14:27:50.235160       1 shared_informer.go:356] "Caches are synced" controller="ipallocator-repair-controller"
I1204 14:27:50.311988       1 shared_informer.go:356] "Caches are synced" controller="cluster_authentication_trust_controller"
I1204 14:27:50.312029       1 shared_informer.go:356] "Caches are synced" controller="node_authorizer"
I1204 14:27:50.312893       1 shared_informer.go:356] "Caches are synced" controller="kubernetes-service-cidr-controller"
I1204 14:27:50.312943       1 default_servicecidr_controller.go:137] Shutting down kubernetes-service-cidr-controller
I1204 14:27:50.324383       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I1204 14:27:50.415682       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I1204 14:27:50.415682       1 controller.go:667] quota admission added evaluator for: serviceaccounts
E1204 14:27:50.512684       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I1204 14:27:50.949052       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1204 14:27:59.148115       1 controller.go:667] quota admission added evaluator for: endpoints
I1204 14:27:59.150577       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I1204 14:27:59.151634       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1204 14:27:59.213175       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I1204 14:27:59.322135       1 controller.go:667] quota admission added evaluator for: deployments.apps
I1204 14:28:20.755717       1 controller.go:667] quota admission added evaluator for: namespaces
I1204 14:28:22.061982       1 alloc.go:328] "allocated clusterIPs" service="recipes-app/recipes-db" clusterIPs={"IPv4":"10.108.223.214"}
I1204 14:28:22.624860       1 alloc.go:328] "allocated clusterIPs" service="recipes-app/recipes-backend" clusterIPs={"IPv4":"10.105.44.76"}
I1204 14:28:23.141273       1 alloc.go:328] "allocated clusterIPs" service="recipes-app/recipes-frontend" clusterIPs={"IPv4":"10.107.188.36"}
I1204 14:28:58.635987       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1204 14:29:05.406277       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-controller-manager [61fcd45eaa15] <==
I1127 22:54:14.964454       1 shared_informer.go:349] "Waiting for caches to sync" controller="PV protection"
I1127 22:54:15.063239       1 controllermanager.go:781] "Started controller" controller="resourceclaim-controller"
I1127 22:54:15.063282       1 controller.go:397] "Starting resource claim controller" logger="resourceclaim-controller"
I1127 22:54:15.063935       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource_claim"
I1127 22:54:15.075316       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I1127 22:54:15.171914       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I1127 22:54:15.178721       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I1127 22:54:15.368147       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I1127 22:54:15.370822       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I1127 22:54:15.371573       1 shared_informer.go:356] "Caches are synced" controller="service account"
I1127 22:54:15.384479       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I1127 22:54:15.463973       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I1127 22:54:15.464278       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I1127 22:54:15.467266       1 shared_informer.go:356] "Caches are synced" controller="node"
I1127 22:54:15.468012       1 shared_informer.go:356] "Caches are synced" controller="expand"
I1127 22:54:15.468336       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I1127 22:54:15.468527       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I1127 22:54:15.469481       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I1127 22:54:15.469499       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I1127 22:54:15.470231       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I1127 22:54:15.472256       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I1127 22:54:15.480514       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I1127 22:54:15.480846       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1127 22:54:15.481241       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I1127 22:54:15.481528       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I1127 22:54:15.481587       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I1127 22:54:15.482133       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I1127 22:54:15.486140       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I1127 22:54:15.486961       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I1127 22:54:15.561699       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I1127 22:54:15.562035       1 shared_informer.go:356] "Caches are synced" controller="job"
I1127 22:54:15.562054       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I1127 22:54:15.562068       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I1127 22:54:15.562078       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I1127 22:54:15.562090       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I1127 22:54:15.562100       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I1127 22:54:15.562116       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I1127 22:54:15.562128       1 shared_informer.go:356] "Caches are synced" controller="GC"
I1127 22:54:15.562136       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I1127 22:54:15.565188       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1127 22:54:15.562141       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I1127 22:54:15.562157       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I1127 22:54:15.564090       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I1127 22:54:15.564129       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I1127 22:54:15.562687       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I1127 22:54:15.566894       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I1127 22:54:15.572502       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I1127 22:54:15.579730       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1127 22:54:15.579898       1 shared_informer.go:356] "Caches are synced" controller="taint"
I1127 22:54:15.580237       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1127 22:54:15.580435       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1127 22:54:15.580509       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1127 22:54:15.579908       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I1127 22:54:15.579923       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I1127 22:54:15.579937       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I1127 22:54:15.582305       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1127 22:54:15.659357       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1127 22:54:15.659418       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I1127 22:54:15.659429       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I1127 23:54:47.697743       1 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." logger="certificatesigningrequest-cleaner-controller" csr="csr-6k7gq" approvedExpiration="1h0m0s"


==> kube-controller-manager [f0bb3fac5c58] <==
I1204 14:27:58.554396       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I1204 14:27:58.554439       1 graph_builder.go:351] "Running" logger="garbage-collector-controller" component="GraphBuilder"
I1204 14:27:58.624137       1 controllermanager.go:781] "Started controller" controller="persistentvolume-binder-controller"
I1204 14:27:58.627801       1 pv_controller_base.go:308] "Starting persistent volume controller" logger="persistentvolume-binder-controller"
I1204 14:27:58.630584       1 shared_informer.go:349] "Waiting for caches to sync" controller="persistent volume"
I1204 14:27:58.713352       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I1204 14:27:58.821833       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I1204 14:27:58.822785       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I1204 14:27:58.823001       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I1204 14:27:58.826332       1 shared_informer.go:356] "Caches are synced" controller="expand"
I1204 14:27:58.826826       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I1204 14:27:58.828710       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I1204 14:27:58.830568       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I1204 14:27:58.831192       1 shared_informer.go:356] "Caches are synced" controller="node"
I1204 14:27:58.831561       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I1204 14:27:58.831886       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I1204 14:27:58.832008       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I1204 14:27:58.832036       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I1204 14:27:58.833397       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1204 14:27:58.836730       1 shared_informer.go:356] "Caches are synced" controller="GC"
I1204 14:27:58.837421       1 shared_informer.go:356] "Caches are synced" controller="taint"
I1204 14:27:58.837679       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1204 14:27:58.837841       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1204 14:27:58.837941       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1204 14:27:58.844713       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I1204 14:27:58.846666       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I1204 14:27:58.845408       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I1204 14:27:58.848468       1 shared_informer.go:356] "Caches are synced" controller="service account"
I1204 14:27:58.848520       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I1204 14:27:58.848635       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I1204 14:27:58.848645       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I1204 14:27:58.850240       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I1204 14:27:58.850528       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I1204 14:27:58.911744       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I1204 14:27:58.914018       1 shared_informer.go:356] "Caches are synced" controller="job"
I1204 14:27:58.923539       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I1204 14:27:58.926551       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I1204 14:27:58.931232       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I1204 14:27:58.931335       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I1204 14:27:58.931733       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I1204 14:27:58.944239       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I1204 14:27:58.944327       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I1204 14:27:58.944347       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I1204 14:27:58.944395       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1204 14:27:58.945733       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I1204 14:27:59.013798       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I1204 14:27:59.013902       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I1204 14:27:59.013977       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I1204 14:27:59.014026       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I1204 14:27:59.014225       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I1204 14:27:59.014321       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I1204 14:27:59.015083       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1204 14:27:59.022918       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I1204 14:27:59.023004       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I1204 14:27:59.023610       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I1204 14:27:59.041444       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I1204 14:27:59.047148       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1204 14:27:59.055071       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1204 14:27:59.055096       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I1204 14:27:59.055107       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"


==> kube-proxy [37a8f50d6d3a] <==
I1204 14:27:58.615299       1 server_linux.go:53] "Using iptables proxy"
I1204 14:27:59.382108       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I1204 14:27:59.483335       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1204 14:27:59.483447       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1204 14:27:59.483594       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1204 14:27:59.556810       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1204 14:27:59.556956       1 server_linux.go:132] "Using iptables Proxier"
I1204 14:27:59.568487       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1204 14:27:59.570144       1 server.go:527] "Version info" version="v1.34.0"
I1204 14:27:59.570258       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1204 14:27:59.575671       1 config.go:200] "Starting service config controller"
I1204 14:27:59.575776       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1204 14:27:59.576805       1 config.go:403] "Starting serviceCIDR config controller"
I1204 14:27:59.576836       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1204 14:27:59.576844       1 config.go:106] "Starting endpoint slice config controller"
I1204 14:27:59.576859       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1204 14:27:59.581670       1 config.go:309] "Starting node config controller"
I1204 14:27:59.581727       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1204 14:27:59.581746       1 shared_informer.go:356] "Caches are synced" controller="node config"
I1204 14:27:59.676625       1 shared_informer.go:356] "Caches are synced" controller="service config"
I1204 14:27:59.677916       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I1204 14:27:59.678144       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"


==> kube-proxy [df243a4dfa8c] <==
I1127 22:54:15.084430       1 server_linux.go:53] "Using iptables proxy"
I1127 22:54:15.983260       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I1127 22:54:16.084743       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1127 22:54:16.084963       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1127 22:54:16.085200       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1127 22:54:16.194254       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1127 22:54:16.194381       1 server_linux.go:132] "Using iptables Proxier"
I1127 22:54:16.260511       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1127 22:54:16.263549       1 server.go:527] "Version info" version="v1.34.0"
I1127 22:54:16.263940       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1127 22:54:16.281126       1 config.go:200] "Starting service config controller"
I1127 22:54:16.281167       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1127 22:54:16.281254       1 config.go:106] "Starting endpoint slice config controller"
I1127 22:54:16.281268       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1127 22:54:16.281295       1 config.go:403] "Starting serviceCIDR config controller"
I1127 22:54:16.281303       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1127 22:54:16.283257       1 config.go:309] "Starting node config controller"
I1127 22:54:16.283696       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1127 22:54:16.381511       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I1127 22:54:16.381553       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I1127 22:54:16.381511       1 shared_informer.go:356] "Caches are synced" controller="service config"
I1127 22:54:16.385341       1 shared_informer.go:356] "Caches are synced" controller="node config"


==> kube-scheduler [b11f75816860] <==
I1204 14:27:45.871414       1 serving.go:386] Generated self-signed cert in-memory
W1204 14:27:50.229547       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1204 14:27:50.229628       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1204 14:27:50.229647       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W1204 14:27:50.229668       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1204 14:27:50.540436       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I1204 14:27:50.540503       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1204 14:27:50.621441       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I1204 14:27:50.621661       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1204 14:27:50.622526       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1204 14:27:50.622998       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1204 14:27:50.734324       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kube-scheduler [d2e776251cf7] <==
I1127 22:54:05.124045       1 serving.go:386] Generated self-signed cert in-memory
I1127 22:54:08.496587       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I1127 22:54:08.572596       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1127 22:54:08.701737       1 requestheader_controller.go:180] Starting RequestHeaderAuthRequestController
I1127 22:54:08.701818       1 shared_informer.go:349] "Waiting for caches to sync" controller="RequestHeaderAuthRequestController"
I1127 22:54:08.705962       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I1127 22:54:08.706491       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1127 22:54:08.707198       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I1127 22:54:08.707239       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I1127 22:54:08.772402       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1127 22:54:08.774715       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1127 22:54:08.873181       1 shared_informer.go:356] "Caches are synced" controller="RequestHeaderAuthRequestController"
I1127 22:54:08.873345       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I1127 22:54:08.874812       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Dec 04 14:27:50 minikube kubelet[1384]: E1204 14:27:50.645541    1384 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Dec 04 14:27:50 minikube kubelet[1384]: I1204 14:27:50.645590    1384 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Dec 04 14:27:50 minikube kubelet[1384]: I1204 14:27:50.714319    1384 kubelet_network.go:47] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Dec 04 14:27:50 minikube kubelet[1384]: E1204 14:27:50.926326    1384 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Dec 04 14:27:50 minikube kubelet[1384]: I1204 14:27:50.927864    1384 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Dec 04 14:27:51 minikube kubelet[1384]: E1204 14:27:51.038593    1384 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Dec 04 14:27:53 minikube kubelet[1384]: I1204 14:27:53.442053    1384 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="d10ec2965fc80e46a08e9c5728484c6f87fa7c24be776681a41a17b9e4d9c858"
Dec 04 14:27:54 minikube kubelet[1384]: I1204 14:27:54.035091    1384 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="b21df9355256b8a9f8f464133c0b4192168cb2eaf567a4db3a60422c422250dd"
Dec 04 14:27:54 minikube kubelet[1384]: I1204 14:27:54.235019    1384 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="2347e858cead5d9cc487869d33177f9513edf91a31ffaf37844c6914afcc2bb9"
Dec 04 14:27:55 minikube kubelet[1384]: I1204 14:27:55.938572    1384 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="c62c989c1e0c46160be737ec381713d48d7d2e11cfbf41b674c0a8861652066a"
Dec 04 14:27:56 minikube kubelet[1384]: I1204 14:27:56.133893    1384 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="5c018bb65dab7b47ad1585fbf5e9b2dc53685ba6ba968716cedd70fc8234fb89"
Dec 04 14:27:58 minikube kubelet[1384]: I1204 14:27:58.121349    1384 scope.go:117] "RemoveContainer" containerID="c31a980d3d090dd3ac01ba14a738b0147628e02962a7ca5e07b8b20c17c1c00c"
Dec 04 14:27:58 minikube kubelet[1384]: E1204 14:27:58.121588    1384 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(b4944b7c-73a2-4722-ba88-d99e38a8e1ba)\"" pod="kube-system/storage-provisioner" podUID="b4944b7c-73a2-4722-ba88-d99e38a8e1ba"
Dec 04 14:27:59 minikube kubelet[1384]: I1204 14:27:59.962502    1384 scope.go:117] "RemoveContainer" containerID="a68f8deabadf0029cd75db13eebc3fe45d3173413474311c7859af7de3ab8a77"
Dec 04 14:27:59 minikube kubelet[1384]: I1204 14:27:59.962665    1384 scope.go:117] "RemoveContainer" containerID="c31a980d3d090dd3ac01ba14a738b0147628e02962a7ca5e07b8b20c17c1c00c"
Dec 04 14:27:59 minikube kubelet[1384]: E1204 14:27:59.962813    1384 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(b4944b7c-73a2-4722-ba88-d99e38a8e1ba)\"" pod="kube-system/storage-provisioner" podUID="b4944b7c-73a2-4722-ba88-d99e38a8e1ba"
Dec 04 14:28:00 minikube kubelet[1384]: I1204 14:28:00.997475    1384 scope.go:117] "RemoveContainer" containerID="c31a980d3d090dd3ac01ba14a738b0147628e02962a7ca5e07b8b20c17c1c00c"
Dec 04 14:28:00 minikube kubelet[1384]: E1204 14:28:00.997790    1384 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(b4944b7c-73a2-4722-ba88-d99e38a8e1ba)\"" pod="kube-system/storage-provisioner" podUID="b4944b7c-73a2-4722-ba88-d99e38a8e1ba"
Dec 04 14:28:16 minikube kubelet[1384]: I1204 14:28:16.312829    1384 scope.go:117] "RemoveContainer" containerID="c31a980d3d090dd3ac01ba14a738b0147628e02962a7ca5e07b8b20c17c1c00c"
Dec 04 14:28:21 minikube kubelet[1384]: I1204 14:28:21.869540    1384 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"init-script\" (UniqueName: \"kubernetes.io/configmap/3b192958-a9cd-4903-b33f-98500f8aec0d-init-script\") pod \"recipes-db-7467f7949b-jf2qx\" (UID: \"3b192958-a9cd-4903-b33f-98500f8aec0d\") " pod="recipes-app/recipes-db-7467f7949b-jf2qx"
Dec 04 14:28:21 minikube kubelet[1384]: I1204 14:28:21.869623    1384 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-klxwg\" (UniqueName: \"kubernetes.io/projected/3b192958-a9cd-4903-b33f-98500f8aec0d-kube-api-access-klxwg\") pod \"recipes-db-7467f7949b-jf2qx\" (UID: \"3b192958-a9cd-4903-b33f-98500f8aec0d\") " pod="recipes-app/recipes-db-7467f7949b-jf2qx"
Dec 04 14:28:21 minikube kubelet[1384]: I1204 14:28:21.869663    1384 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"postgres-storage\" (UniqueName: \"kubernetes.io/empty-dir/3b192958-a9cd-4903-b33f-98500f8aec0d-postgres-storage\") pod \"recipes-db-7467f7949b-jf2qx\" (UID: \"3b192958-a9cd-4903-b33f-98500f8aec0d\") " pod="recipes-app/recipes-db-7467f7949b-jf2qx"
Dec 04 14:28:22 minikube kubelet[1384]: I1204 14:28:22.475648    1384 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-s7nn9\" (UniqueName: \"kubernetes.io/projected/7ad7fa93-70f4-4240-b477-85dfd304db9c-kube-api-access-s7nn9\") pod \"recipes-backend-5447f78f68-95mgh\" (UID: \"7ad7fa93-70f4-4240-b477-85dfd304db9c\") " pod="recipes-app/recipes-backend-5447f78f68-95mgh"
Dec 04 14:28:22 minikube kubelet[1384]: I1204 14:28:22.509194    1384 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="4fa62a3f33be18b825d0f72d262454701ba7e63e59f7f9e478c4682cdb571007"
Dec 04 14:28:23 minikube kubelet[1384]: I1204 14:28:23.114709    1384 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-4ggqm\" (UniqueName: \"kubernetes.io/projected/ccff71ef-7825-4eae-85cf-89cb9bd9617a-kube-api-access-4ggqm\") pod \"recipes-frontend-d7c9dd5b9-b9cj2\" (UID: \"ccff71ef-7825-4eae-85cf-89cb9bd9617a\") " pod="recipes-app/recipes-frontend-d7c9dd5b9-b9cj2"
Dec 04 14:28:23 minikube kubelet[1384]: E1204 14:28:23.370262    1384 kuberuntime_manager.go:1449] "Unhandled Error" err="container backend start failed in pod recipes-backend-5447f78f68-95mgh_recipes-app(7ad7fa93-70f4-4240-b477-85dfd304db9c): ErrImageNeverPull: Container image \"three_tier_recipes_system-backend:latest\" is not present with pull policy of Never" logger="UnhandledError"
Dec 04 14:28:23 minikube kubelet[1384]: E1204 14:28:23.370347    1384 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend\" with ErrImageNeverPull: \"Container image \\\"three_tier_recipes_system-backend:latest\\\" is not present with pull policy of Never\"" pod="recipes-app/recipes-backend-5447f78f68-95mgh" podUID="7ad7fa93-70f4-4240-b477-85dfd304db9c"
Dec 04 14:28:23 minikube kubelet[1384]: E1204 14:28:23.577443    1384 kuberuntime_manager.go:1449] "Unhandled Error" err="container backend start failed in pod recipes-backend-5447f78f68-95mgh_recipes-app(7ad7fa93-70f4-4240-b477-85dfd304db9c): ErrImageNeverPull: Container image \"three_tier_recipes_system-backend:latest\" is not present with pull policy of Never" logger="UnhandledError"
Dec 04 14:28:23 minikube kubelet[1384]: E1204 14:28:23.577520    1384 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend\" with ErrImageNeverPull: \"Container image \\\"three_tier_recipes_system-backend:latest\\\" is not present with pull policy of Never\"" pod="recipes-app/recipes-backend-5447f78f68-95mgh" podUID="7ad7fa93-70f4-4240-b477-85dfd304db9c"
Dec 04 14:28:24 minikube kubelet[1384]: E1204 14:28:24.045301    1384 kuberuntime_manager.go:1449] "Unhandled Error" err="container frontend start failed in pod recipes-frontend-d7c9dd5b9-b9cj2_recipes-app(ccff71ef-7825-4eae-85cf-89cb9bd9617a): ErrImageNeverPull: Container image \"three_tier_recipes_system-frontend:latest\" is not present with pull policy of Never" logger="UnhandledError"
Dec 04 14:28:24 minikube kubelet[1384]: E1204 14:28:24.045389    1384 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with ErrImageNeverPull: \"Container image \\\"three_tier_recipes_system-frontend:latest\\\" is not present with pull policy of Never\"" pod="recipes-app/recipes-frontend-d7c9dd5b9-b9cj2" podUID="ccff71ef-7825-4eae-85cf-89cb9bd9617a"
Dec 04 14:28:24 minikube kubelet[1384]: E1204 14:28:24.625746    1384 kuberuntime_manager.go:1449] "Unhandled Error" err="container frontend start failed in pod recipes-frontend-d7c9dd5b9-b9cj2_recipes-app(ccff71ef-7825-4eae-85cf-89cb9bd9617a): ErrImageNeverPull: Container image \"three_tier_recipes_system-frontend:latest\" is not present with pull policy of Never" logger="UnhandledError"
Dec 04 14:28:24 minikube kubelet[1384]: E1204 14:28:24.625950    1384 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with ErrImageNeverPull: \"Container image \\\"three_tier_recipes_system-frontend:latest\\\" is not present with pull policy of Never\"" pod="recipes-app/recipes-frontend-d7c9dd5b9-b9cj2" podUID="ccff71ef-7825-4eae-85cf-89cb9bd9617a"
Dec 04 14:28:35 minikube kubelet[1384]: E1204 14:28:35.313821    1384 kuberuntime_manager.go:1449] "Unhandled Error" err="container frontend start failed in pod recipes-frontend-d7c9dd5b9-b9cj2_recipes-app(ccff71ef-7825-4eae-85cf-89cb9bd9617a): ErrImageNeverPull: Container image \"three_tier_recipes_system-frontend:latest\" is not present with pull policy of Never" logger="UnhandledError"
Dec 04 14:28:35 minikube kubelet[1384]: E1204 14:28:35.313936    1384 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with ErrImageNeverPull: \"Container image \\\"three_tier_recipes_system-frontend:latest\\\" is not present with pull policy of Never\"" pod="recipes-app/recipes-frontend-d7c9dd5b9-b9cj2" podUID="ccff71ef-7825-4eae-85cf-89cb9bd9617a"
Dec 04 14:28:38 minikube kubelet[1384]: E1204 14:28:38.316862    1384 kuberuntime_manager.go:1449] "Unhandled Error" err="container backend start failed in pod recipes-backend-5447f78f68-95mgh_recipes-app(7ad7fa93-70f4-4240-b477-85dfd304db9c): ErrImageNeverPull: Container image \"three_tier_recipes_system-backend:latest\" is not present with pull policy of Never" logger="UnhandledError"
Dec 04 14:28:38 minikube kubelet[1384]: E1204 14:28:38.316986    1384 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend\" with ErrImageNeverPull: \"Container image \\\"three_tier_recipes_system-backend:latest\\\" is not present with pull policy of Never\"" pod="recipes-app/recipes-backend-5447f78f68-95mgh" podUID="7ad7fa93-70f4-4240-b477-85dfd304db9c"
Dec 04 14:28:48 minikube kubelet[1384]: E1204 14:28:48.316879    1384 kuberuntime_manager.go:1449] "Unhandled Error" err="container frontend start failed in pod recipes-frontend-d7c9dd5b9-b9cj2_recipes-app(ccff71ef-7825-4eae-85cf-89cb9bd9617a): ErrImageNeverPull: Container image \"three_tier_recipes_system-frontend:latest\" is not present with pull policy of Never" logger="UnhandledError"
Dec 04 14:28:48 minikube kubelet[1384]: E1204 14:28:48.316994    1384 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with ErrImageNeverPull: \"Container image \\\"three_tier_recipes_system-frontend:latest\\\" is not present with pull policy of Never\"" pod="recipes-app/recipes-frontend-d7c9dd5b9-b9cj2" podUID="ccff71ef-7825-4eae-85cf-89cb9bd9617a"
Dec 04 14:28:53 minikube kubelet[1384]: E1204 14:28:53.314114    1384 kuberuntime_manager.go:1449] "Unhandled Error" err="container backend start failed in pod recipes-backend-5447f78f68-95mgh_recipes-app(7ad7fa93-70f4-4240-b477-85dfd304db9c): ErrImageNeverPull: Container image \"three_tier_recipes_system-backend:latest\" is not present with pull policy of Never" logger="UnhandledError"
Dec 04 14:28:53 minikube kubelet[1384]: E1204 14:28:53.314642    1384 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend\" with ErrImageNeverPull: \"Container image \\\"three_tier_recipes_system-backend:latest\\\" is not present with pull policy of Never\"" pod="recipes-app/recipes-backend-5447f78f68-95mgh" podUID="7ad7fa93-70f4-4240-b477-85dfd304db9c"
Dec 04 14:29:00 minikube kubelet[1384]: E1204 14:29:00.312804    1384 kuberuntime_manager.go:1449] "Unhandled Error" err="container frontend start failed in pod recipes-frontend-d7c9dd5b9-b9cj2_recipes-app(ccff71ef-7825-4eae-85cf-89cb9bd9617a): ErrImageNeverPull: Container image \"three_tier_recipes_system-frontend:latest\" is not present with pull policy of Never" logger="UnhandledError"
Dec 04 14:29:00 minikube kubelet[1384]: E1204 14:29:00.312908    1384 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with ErrImageNeverPull: \"Container image \\\"three_tier_recipes_system-frontend:latest\\\" is not present with pull policy of Never\"" pod="recipes-app/recipes-frontend-d7c9dd5b9-b9cj2" podUID="ccff71ef-7825-4eae-85cf-89cb9bd9617a"
Dec 04 14:29:08 minikube kubelet[1384]: E1204 14:29:08.312630    1384 kuberuntime_manager.go:1449] "Unhandled Error" err="container backend start failed in pod recipes-backend-5447f78f68-95mgh_recipes-app(7ad7fa93-70f4-4240-b477-85dfd304db9c): ErrImageNeverPull: Container image \"three_tier_recipes_system-backend:latest\" is not present with pull policy of Never" logger="UnhandledError"
Dec 04 14:29:08 minikube kubelet[1384]: E1204 14:29:08.312724    1384 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend\" with ErrImageNeverPull: \"Container image \\\"three_tier_recipes_system-backend:latest\\\" is not present with pull policy of Never\"" pod="recipes-app/recipes-backend-5447f78f68-95mgh" podUID="7ad7fa93-70f4-4240-b477-85dfd304db9c"
Dec 04 14:29:12 minikube kubelet[1384]: E1204 14:29:12.314574    1384 kuberuntime_manager.go:1449] "Unhandled Error" err="container frontend start failed in pod recipes-frontend-d7c9dd5b9-b9cj2_recipes-app(ccff71ef-7825-4eae-85cf-89cb9bd9617a): ErrImageNeverPull: Container image \"three_tier_recipes_system-frontend:latest\" is not present with pull policy of Never" logger="UnhandledError"
Dec 04 14:29:12 minikube kubelet[1384]: E1204 14:29:12.314741    1384 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with ErrImageNeverPull: \"Container image \\\"three_tier_recipes_system-frontend:latest\\\" is not present with pull policy of Never\"" pod="recipes-app/recipes-frontend-d7c9dd5b9-b9cj2" podUID="ccff71ef-7825-4eae-85cf-89cb9bd9617a"
Dec 04 14:29:13 minikube kubelet[1384]: I1204 14:29:13.757670    1384 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="recipes-app/recipes-db-7467f7949b-jf2qx" podStartSLOduration=2.780797501 podStartE2EDuration="52.757641037s" podCreationTimestamp="2025-12-04 14:28:21 +0000 UTC" firstStartedPulling="2025-12-04 14:28:22.603333287 +0000 UTC m=+45.967529427" lastFinishedPulling="2025-12-04 14:29:12.578521805 +0000 UTC m=+95.944372963" observedRunningTime="2025-12-04 14:29:13.757217503 +0000 UTC m=+97.123068661" watchObservedRunningTime="2025-12-04 14:29:13.757641037 +0000 UTC m=+97.123492295"
Dec 04 14:29:21 minikube kubelet[1384]: E1204 14:29:21.316886    1384 kuberuntime_manager.go:1449] "Unhandled Error" err="container backend start failed in pod recipes-backend-5447f78f68-95mgh_recipes-app(7ad7fa93-70f4-4240-b477-85dfd304db9c): ErrImageNeverPull: Container image \"three_tier_recipes_system-backend:latest\" is not present with pull policy of Never" logger="UnhandledError"
Dec 04 14:29:21 minikube kubelet[1384]: E1204 14:29:21.317213    1384 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend\" with ErrImageNeverPull: \"Container image \\\"three_tier_recipes_system-backend:latest\\\" is not present with pull policy of Never\"" pod="recipes-app/recipes-backend-5447f78f68-95mgh" podUID="7ad7fa93-70f4-4240-b477-85dfd304db9c"
Dec 04 14:29:24 minikube kubelet[1384]: E1204 14:29:24.309853    1384 kuberuntime_manager.go:1449] "Unhandled Error" err="container frontend start failed in pod recipes-frontend-d7c9dd5b9-b9cj2_recipes-app(ccff71ef-7825-4eae-85cf-89cb9bd9617a): ErrImageNeverPull: Container image \"three_tier_recipes_system-frontend:latest\" is not present with pull policy of Never" logger="UnhandledError"
Dec 04 14:29:24 minikube kubelet[1384]: E1204 14:29:24.309939    1384 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with ErrImageNeverPull: \"Container image \\\"three_tier_recipes_system-frontend:latest\\\" is not present with pull policy of Never\"" pod="recipes-app/recipes-frontend-d7c9dd5b9-b9cj2" podUID="ccff71ef-7825-4eae-85cf-89cb9bd9617a"
Dec 04 14:29:35 minikube kubelet[1384]: E1204 14:29:35.345219    1384 kuberuntime_manager.go:1449] "Unhandled Error" err="container frontend start failed in pod recipes-frontend-d7c9dd5b9-b9cj2_recipes-app(ccff71ef-7825-4eae-85cf-89cb9bd9617a): ErrImageNeverPull: Container image \"three_tier_recipes_system-frontend:latest\" is not present with pull policy of Never" logger="UnhandledError"
Dec 04 14:29:35 minikube kubelet[1384]: E1204 14:29:35.345340    1384 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with ErrImageNeverPull: \"Container image \\\"three_tier_recipes_system-frontend:latest\\\" is not present with pull policy of Never\"" pod="recipes-app/recipes-frontend-d7c9dd5b9-b9cj2" podUID="ccff71ef-7825-4eae-85cf-89cb9bd9617a"
Dec 04 14:29:36 minikube kubelet[1384]: E1204 14:29:36.315917    1384 kuberuntime_manager.go:1449] "Unhandled Error" err="container backend start failed in pod recipes-backend-5447f78f68-95mgh_recipes-app(7ad7fa93-70f4-4240-b477-85dfd304db9c): ErrImageNeverPull: Container image \"three_tier_recipes_system-backend:latest\" is not present with pull policy of Never" logger="UnhandledError"
Dec 04 14:29:36 minikube kubelet[1384]: E1204 14:29:36.316431    1384 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend\" with ErrImageNeverPull: \"Container image \\\"three_tier_recipes_system-backend:latest\\\" is not present with pull policy of Never\"" pod="recipes-app/recipes-backend-5447f78f68-95mgh" podUID="7ad7fa93-70f4-4240-b477-85dfd304db9c"
Dec 04 14:29:48 minikube kubelet[1384]: E1204 14:29:48.311678    1384 kuberuntime_manager.go:1449] "Unhandled Error" err="container frontend start failed in pod recipes-frontend-d7c9dd5b9-b9cj2_recipes-app(ccff71ef-7825-4eae-85cf-89cb9bd9617a): ErrImageNeverPull: Container image \"three_tier_recipes_system-frontend:latest\" is not present with pull policy of Never" logger="UnhandledError"
Dec 04 14:29:48 minikube kubelet[1384]: E1204 14:29:48.311817    1384 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend\" with ErrImageNeverPull: \"Container image \\\"three_tier_recipes_system-frontend:latest\\\" is not present with pull policy of Never\"" pod="recipes-app/recipes-frontend-d7c9dd5b9-b9cj2" podUID="ccff71ef-7825-4eae-85cf-89cb9bd9617a"
Dec 04 14:29:51 minikube kubelet[1384]: E1204 14:29:51.328895    1384 kuberuntime_manager.go:1449] "Unhandled Error" err="container backend start failed in pod recipes-backend-5447f78f68-95mgh_recipes-app(7ad7fa93-70f4-4240-b477-85dfd304db9c): ErrImageNeverPull: Container image \"three_tier_recipes_system-backend:latest\" is not present with pull policy of Never" logger="UnhandledError"
Dec 04 14:29:51 minikube kubelet[1384]: E1204 14:29:51.329106    1384 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend\" with ErrImageNeverPull: \"Container image \\\"three_tier_recipes_system-backend:latest\\\" is not present with pull policy of Never\"" pod="recipes-app/recipes-backend-5447f78f68-95mgh" podUID="7ad7fa93-70f4-4240-b477-85dfd304db9c"


==> storage-provisioner [452c084c53b3] <==
W1204 14:29:02.509551       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:02.607457       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:04.620385       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:04.643317       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:06.656225       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:06.706398       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:08.713406       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:08.731013       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:10.735179       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:10.785555       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:12.792188       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:12.855477       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:14.861994       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:14.870770       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:16.873410       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:16.882935       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:18.886913       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:18.893857       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:20.897670       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:20.906180       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:22.908847       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:22.916674       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:24.920155       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:24.930588       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:26.937169       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:26.946873       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:28.950728       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:28.958654       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:30.972784       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:31.018025       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:33.023846       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:33.038717       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:35.045196       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:35.056234       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:37.068959       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:37.087951       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:39.094332       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:39.110680       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:41.117704       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:41.126605       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:43.132387       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:43.143480       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:45.146912       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:45.157031       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:47.167502       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:47.206657       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:49.213073       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:49.224655       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:51.231641       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:51.240558       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:53.246592       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:53.289615       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:55.296959       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:55.313232       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:57.329517       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:57.402384       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:59.410110       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:29:59.446697       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:30:01.450576       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1204 14:30:01.458584       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice


==> storage-provisioner [c31a980d3d09] <==
I1204 14:27:56.720339       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1204 14:27:56.931224       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": x509: certificate signed by unknown authority

